[2024-01-13 20:42:07,693] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
2024-01-13 20:42:10.539551: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-01-13 20:42:10.586203: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-13 20:42:10.586242: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-13 20:42:10.586286: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-13 20:42:10.595217: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-01-13 20:42:10.595542: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-13 20:42:11.629047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
01/13/2024 20:42:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
01/13/2024 20:42:16 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/test-202401132034-128-5e-3/runs/Jan13_20-42-13_dsw-305162-bfbfdc4c5-m6whp,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=300,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=output/test-202401132034-128-5e-3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=True,
run_name=output/test-202401132034-128-5e-3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:737] 2024-01-13 20:42:16,297 >> loading configuration file /mnt/workspace/.cache/modelscope/ZhipuAI/chatglm3-6b/config.json
[INFO|configuration_utils.py:737] 2024-01-13 20:42:16,299 >> loading configuration file /mnt/workspace/.cache/modelscope/ZhipuAI/chatglm3-6b/config.json
[INFO|configuration_utils.py:802] 2024-01-13 20:42:16,300 >> Model config ChatGLMConfig {
  "_name_or_path": "/mnt/workspace/.cache/modelscope/ZhipuAI/chatglm3-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 8192,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|tokenization_utils_base.py:2024] 2024-01-13 20:42:16,303 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2024] 2024-01-13 20:42:16,303 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2024] 2024-01-13 20:42:16,303 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2024] 2024-01-13 20:42:16,303 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2024] 2024-01-13 20:42:16,303 >> loading file tokenizer.json
[INFO|modeling_utils.py:3341] 2024-01-13 20:42:16,571 >> loading weights file /mnt/workspace/.cache/modelscope/ZhipuAI/chatglm3-6b/pytorch_model.bin.index.json
[INFO|configuration_utils.py:826] 2024-01-13 20:42:16,572 >> Generate config GenerationConfig {
  "eos_token_id": 2,
  "pad_token_id": 0,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.81s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:09,  1.92s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:07,  1.93s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:07<00:05,  1.90s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:09<00:03,  1.92s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:11<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:12<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:12<00:00,  1.81s/it]
[INFO|modeling_utils.py:4185] 2024-01-13 20:42:29,271 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[WARNING|modeling_utils.py:4187] 2024-01-13 20:42:29,271 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /mnt/workspace/.cache/modelscope/ZhipuAI/chatglm3-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:3751] 2024-01-13 20:42:29,273 >> Generation config file not found, using a generation config created from the model config.
Sanity Check >>>>>>>>>>>>>
           '[gMASK]':  64790 ->   -100
               'sop':  64792 ->   -100
        '<|system|>':  64794 ->   -100
                  '':  30910 ->   -100
                '\n':     13 ->   -100
                  '':  30910 ->   -100
                '背景':  32797 ->   -100
                '知识':  31848 ->   -100
                 '：':  31211 ->   -100
                '查询':  34262 ->   -100
                '会给':  44185 ->   -100
                 '定':  54621 ->   -100
                '一个':  31623 ->   -100
                 'x':  30948 ->   -100
                'ls':   7463 ->   -100
                 'x':  30948 ->   -100
               '文件的':  52483 ->   -100
                '路径':  35816 ->   -100
                 '，':  31123 ->   -100
                '包含':  33302 ->   -100
                 '的':  54530 ->   -100
                 '各':  54752 ->   -100
                 '张':  54940 ->   -100
                '工作':  31624 ->   -100
                 '表':  54670 ->   -100
             'sheet':  20352 ->   -100
                 '的':  54530 ->   -100
                '名称':  33624 ->   -100
                 '，':  31123 ->   -100
                '数据':  31786 ->   -100
                 '列':  55065 ->   -100
                 '名':  54653 ->   -100
                 '从':  54708 ->   -100
                 '第':  54616 ->   -100
                 '几':  55013 ->   -100
                 '行':  54560 ->   -100
                '开始':  31699 ->   -100
                 '，':  31123 ->   -100
                '行列':  42559 ->   -100
               '有哪些':  34953 ->   -100
                '属性':  37027 ->   -100
                 '，':  31123 ->   -100
                 '请':  55073 ->   -100
                '根据':  31793 ->   -100
               '下面的':  42603 ->   -100
                '查询':  34262 ->   -100
                '要求':  31696 ->   -100
                 '，':  31123 ->   -100
                '输出':  35898 ->   -100
                '格式':  36844 ->   -100
                 '和':  54542 ->   -100
                '结果':  31951 ->   -100
               '正确的':  34897 ->   -100
                 '可':  54568 ->   -100
                '执行':  32101 ->   -100
            'python':  23720 ->   -100
                '代码':  35089 ->   -100
          '<|user|>':  64795 ->   -100
                  '':  30910 ->   -100
                '\n':     13 ->   -100
                 '.':    918 ->   -100
                 '/':  30967 ->   -100
              'file':   3743 ->   -100
                 '/':  30967 ->   -100
                '民用':  41801 ->   -100
                '汽车':  32031 ->   -100
                '拥有':  32104 ->   -100
                 '量':  54698 ->   -100
                '汇总':  39060 ->   -100
                 '.':  30930 ->   -100
                 'x':  30948 ->   -100
                'ls':   7463 ->   -100
                 'x':  30948 ->   -100
                '文件':  32410 ->   -100
                 '中':  54538 ->   -100
                '文件':  32410 ->   -100
                '中共':  32601 ->   -100
                 '有':  54536 ->   -100
                 '4':  30972 ->   -100
                 '个':  54550 ->   -100
               '独立的':  41082 ->   -100
             'sheet':  20352 ->   -100
                 '，':  31123 ->   -100
               '分别为':  35222 ->   -100
                 '“':  30989 ->   -100
                '民用':  41801 ->   -100
                '汽车':  32031 ->   -100
                '拥有':  32104 ->   -100
                 '量':  54698 ->   -100
                '汇总':  39060 ->   -100
               '”、“':  32442 ->   -100
                '民用':  41801 ->   -100
                 '载':  55387 ->   -100
                 '货':  55466 ->   -100
               '”、“':  32442 ->   -100
                '民用':  41801 ->   -100
                 '载':  55387 ->   -100
                 '客':  54992 ->   -100
               '”、“':  32442 ->   -100
                '民用':  41801 ->   -100
                '其他':  31722 ->   -100
               '”。“':  41969 ->   -100
                '民用':  41801 ->   -100
                '汽车':  32031 ->   -100
                '拥有':  32104 ->   -100
                 '量':  54698 ->   -100
                '汇总':  39060 ->   -100
                 '”':  30991 ->   -100
                 '的':  54530 ->   -100
                 '列':  55065 ->   -100
                 '名':  54653 ->   -100
                 '从':  54708 ->   -100
                 '第':  54616 ->   -100
                 '5':  30970 ->   -100
                 '行':  54560 ->   -100
                '开始':  31699 ->   -100
                '，“':  31771 ->   -100
                '民用':  41801 ->   -100
                 '载':  55387 ->   -100
                 '货':  55466 ->   -100
               '”、“':  32442 ->   -100
                '民用':  41801 ->   -100
                 '载':  55387 ->   -100
                 '客':  54992 ->   -100
                 '”':  30991 ->   -100
                 '的':  54530 ->   -100
                 '列':  55065 ->   -100
                 '名':  54653 ->   -100
                 '从':  54708 ->   -100
                 '第':  54616 ->   -100
                 '4':  30972 ->   -100
                 '行':  54560 ->   -100
                '开始':  31699 ->   -100
                 '，':  31123 ->   -100
                 '列':  55065 ->   -100
                 '名':  54653 ->   -100
                 '有':  54536 ->   -100
                '省份':  38063 ->   -100
                 '、':  31201 ->   -100
                 '2':  30943 ->   -100
                 '0':  30940 ->   -100
                 '0':  30940 ->   -100
                 '2':  30943 ->   -100
                 '年':  54540 ->   -100
                 '、':  31201 ->   -100
                 '2':  30943 ->   -100
                 '0':  30940 ->   -100
                 '0':  30940 ->   -100
                 '3':  30966 ->   -100
                 '年':  54540 ->   -100
                '……':   6512 ->   -100
               '一直到':  44188 ->   -100
                 '2':  30943 ->   -100
                 '0':  30940 ->   -100
                 '2':  30943 ->   -100
                 '0':  30940 ->   -100
                 '年':  54540 ->   -100
                 '等':  54609 ->   -100
                 '项':  54794 ->   -100
                 '，':  31123 ->   -100
               '不存在':  39974 ->   -100
                '汇总':  39060 ->   -100
                '各个':  33402 ->   -100
                '省份':  38063 ->   -100
                 '的':  54530 ->   -100
                '全国':  31742 ->   -100
                 '项':  54794 ->   -100
                 '。':  31155 ->   -100
                '单位':  31790 ->   -100
                 '为':  54541 ->   -100
                '万辆':  40453 ->   -100
                 '，':  31123 ->   -100
                 '请':  55073 ->   -100
                '查询':  34262 ->   -100
                 '2':  30943 ->   -100
                 '0':  30940 ->   -100
                 '2':  30943 ->   -100
                 '0':  30940 ->   -100
                 '年':  54540 ->   -100
                 '，':  31123 ->   -100
                '北京':  31719 ->   -100
                 '和':  54542 ->   -100
               '上海的':  48750 ->   -100
                '民用':  41801 ->   -100
                '汽车':  32031 ->   -100
                '拥有':  32104 ->   -100
                 '量':  54698 ->   -100
               '分别是':  36733 ->   -100
                '多少':  32326 ->   -100
                 '？':  31514 ->   -100
     '<|assistant|>':  64796 ->   -100
                  '':  30910 ->  30910
                '\n':     13 ->     13
            'import':    937 ->    937
              'pand':  12833 ->  12833
                'as':    298 ->    298
                'as':    362 ->    362
                'pd':  23382 ->  23382
                '\n':     13 ->     13
            'import':  12787 ->  12787
             'numpy':  22215 ->  22215
                'as':    362 ->    362
                'np':   8036 ->   8036
                '\n':     13 ->     13
              'file':   3743 ->   3743
                 '_':  30962 ->  30962
              'path':   3295 ->   3295
                 '=':  30980 ->  30980
                "'.":   6263 ->   6263
                 '/':  30967 ->  30967
              'file':   3743 ->   3743
                 '/':  30967 ->  30967
                '民用':  41801 ->  41801
                '汽车':  32031 ->  32031
                '拥有':  32104 ->  32104
                 '量':  54698 ->  54698
                '汇总':  39060 ->  39060
                 '.':  30930 ->  30930
                 'x':  30948 ->  30948
                'ls':   7463 ->   7463
                 'x':  30948 ->  30948
                 "'":  30953 ->  30953
                '\n':     13 ->     13
              'data':   1166 ->   1166
                 '=':    542 ->    542
                'pd':  23382 ->  23382
                 '.':  30930 ->  30930
              'read':   1042 ->   1042
                 '_':  30962 ->  30962
                'ex':    914 ->    914
               'cel':   4397 ->   4397
                 '(':  30946 ->  30946
              'file':   3743 ->   3743
                 '_':  30962 ->  30962
              'path':   3295 ->   3295
                 ',':  30932 ->  30932
             'sheet':   8791 ->   8791
                 '_':  30962 ->  30962
              'name':   2323 ->   2323
                "='":   6521 ->   6521
                '民用':  41801 ->  41801
                '汽车':  32031 ->  32031
                '拥有':  32104 ->  32104
                 '量':  54698 ->  54698
                '汇总':  39060 ->  39060
                "',":   1589 ->   1589
            'engine':   2809 ->   2809
                "='":   6521 ->   6521
              'open':   7237 ->   7237
                'py':   6433 ->   6433
                 'x':  30948 ->  30948
                 'l':  30920 ->  30920
                "',":   1589 ->   1589
            'header':  12990 ->  12990
                 '=':  30980 ->  30980
                 '4':  30972 ->  30972
                 ')':  30945 ->  30945
                 ' ':    265 ->    265
                 '#':  31010 ->  31010
                 '从':  37708 ->  37708
                '第五':  32771 ->  32771
                 '行':  54560 ->  54560
                '开始':  31699 ->  31699
                 '读':  55116 ->  55116
                 '取':  54891 ->  54891
                '数据':  31786 ->  31786
                '\n':     13 ->     13
                 '#':  31010 ->  31010
                 '要':  43159 ->  43159
                '查询':  34262 ->  34262
               '的城市':  36226 ->  36226
                 '和':  54542 ->  54542
                '年份':  43489 ->  43489
                '\n':     13 ->     13
              'city':  17654 ->  17654
                 '_':  30962 ->  30962
                 '1':  30939 ->  30939
                 '=':    542 ->    542
                 "'":    765 ->    765
                '北京':  31719 ->  31719
                 "'":  30953 ->  30953
                 ' ':    265 ->    265
                 '#':  31010 ->  31010
                  '':  30910 ->  30910
              '可以根据':  43909 ->  43909
                '需要':  31665 ->  31665
                '修改':  33608 ->  33608
                 '为':  54541 ->  54541
                '其他':  31722 ->  31722
                '城市':  31733 ->  31733
                '\n':     13 ->     13
              'city':  17654 ->  17654
                 '_':  30962 ->  30962
                 '2':  30943 ->  30943
                 '=':    542 ->    542
                 "'":    765 ->    765
                '上海':  31770 ->  31770
                 "'":  30953 ->  30953
                 ' ':    265 ->    265
                 '#':  31010 ->  31010
                  '':  30910 ->  30910
              '可以根据':  43909 ->  43909
                '需要':  31665 ->  31665
                '修改':  33608 ->  33608
                 '为':  54541 ->  54541
                '其他':  31722 ->  31722
                '城市':  31733 ->  31733
                '\n':     13 ->     13
              'year':   2475 ->   2475
                 '_':  30962 ->  30962
                'to':   1143 ->   1143
                 '_':  30962 ->  30962
             'check':   7170 ->   7170
                 '=':    542 ->    542
                 "'":    765 ->    765
                 '2':  30943 ->  30943
                 '0':  30940 ->  30940
                 '2':  30943 ->  30943
                 '0':  30940 ->  30940
                 "'":  30953 ->  30953
                 ' ':    265 ->    265
                 '#':  31010 ->  31010
                  '':  30910 ->  30910
              '可以根据':  43909 ->  43909
                '需要':  31665 ->  31665
                '修改':  33608 ->  33608
                 '为':  54541 ->  54541
                '其他':  31722 ->  31722
                '年份':  43489 ->  43489
                '\n':     13 ->     13
                 '#':  31010 ->  31010
                  '':  30910 ->  30910
                '获取':  34073 ->  34073
                '特定':  35743 ->  35743
                '城市':  31733 ->  31733
                 '在':  54534 ->  54534
                '特定':  35743 ->  35743
                '年份':  43489 ->  43489
                 '的':  54530 ->  54530
                '民用':  41801 ->  41801
                '汽车':  32031 ->  32031
                '拥有':  32104 ->  32104
                 '量':  54698 ->  54698
                '\n':     13 ->     13
               'car':   6747 ->   6747
                 '_':  30962 ->  30962
             'count':   5597 ->   5597
                 '_':  30962 ->  30962
              'city':  17654 ->  17654
                 '_':  30962 ->  30962
                 '1':  30939 ->  30939
                 '=':    542 ->    542
              'data':   1166 ->   1166
                 '.':  30930 ->  30930
               'loc':  12687 ->  12687
                 '[':  30995 ->  30995
              'data':   2899 ->   2899
                "['":   4005 ->   4005
                '省份':  38063 ->  38063
                "']":   4960 ->   4960
                '==':   2731 ->   2731
              'city':   1911 ->   1911
                 '_':  30962 ->  30962
                 '1':  30939 ->  30939
                 ',':  30932 ->  30932
                 'f':    279 ->    279
                 "'":  30953 ->  30953
                 '{':  30982 ->  30982
              'year':   2475 ->   2475
                 '_':  30962 ->  30962
                'to':   1143 ->   1143
                 '_':  30962 ->  30962
             'check':   7170 ->   7170
                 '}':  30983 ->  30983
                 '年':  54540 ->  54540
               "'].":  28380 ->  28380
            'values':  12297 ->  12297
                 '[':  30995 ->  30995
                 '0':  30940 ->  30940
                 ']':  30996 ->  30996
                '\n':     13 ->     13
               'car':   6747 ->   6747
                 '_':  30962 ->  30962
             'count':   5597 ->   5597
                 '_':  30962 ->  30962
              'city':  17654 ->  17654
                 '_':  30962 ->  30962
                 '2':  30943 ->  30943
                 '=':    542 ->    542
              'data':   1166 ->   1166
                 '.':  30930 ->  30930
               'loc':  12687 ->  12687
                 '[':  30995 ->  30995
              'data':   2899 ->   2899
                "['":   4005 ->   4005
                '省份':  38063 ->  38063
                "']":   4960 ->   4960
                '==':   2731 ->   2731
              'city':   1911 ->   1911
                 '_':  30962 ->  30962
                 '2':  30943 ->  30943
                 ',':  30932 ->  30932
                 'f':    279 ->    279
                 "'":  30953 ->  30953
                 '{':  30982 ->  30982
              'year':   2475 ->   2475
                 '_':  30962 ->  30962
                'to':   1143 ->   1143
                 '_':  30962 ->  30962
             'check':   7170 ->   7170
                 '}':  30983 ->  30983
                 '年':  54540 ->  54540
               "'].":  28380 ->  28380
            'values':  12297 ->  12297
                 '[':  30995 ->  30995
                 '0':  30940 ->  30940
                 ']':  30996 ->  30996
                '\n':     13 ->     13
             'print':   4466 ->   4466
                 '(':  30946 ->  30946
                 'f':  30926 ->  30926
                 '"':  30955 ->  30955
                 '{':  30982 ->  30982
              'year':   2475 ->   2475
                 '_':  30962 ->  30962
                'to':   1143 ->   1143
                 '_':  30962 ->  30962
             'check':   7170 ->   7170
                 '}':  30983 ->  30983
                 '年':  54540 ->  54540
                 '，':  31123 ->  31123
                 '{':  30982 ->  30982
              'city':  17654 ->  17654
                 '_':  30962 ->  30962
                 '1':  30939 ->  30939
                 '}':  30983 ->  30983
                 '的':  54530 ->  54530
                '民用':  41801 ->  41801
                '汽车':  32031 ->  32031
                '拥有':  32104 ->  32104
                '量为':  44368 ->  44368
                 '：':  31211 ->  31211
                 '{':  30982 ->  30982
               'car':   6747 ->   6747
                 '_':  30962 ->  30962
             'count':   5597 ->   5597
                 '_':  30962 ->  30962
              'city':  17654 ->  17654
                 '_':  30962 ->  30962
                 '1':  30939 ->  30939
                 '}':  30983 ->  30983
                 '万':  38981 ->  38981
                 '辆':  55793 ->  55793
                '")':   2479 ->   2479
                '\n':     13 ->     13
             'print':   4466 ->   4466
                 '(':  30946 ->  30946
                 'f':  30926 ->  30926
                 '"':  30955 ->  30955
                 '{':  30982 ->  30982
              'year':   2475 ->   2475
                 '_':  30962 ->  30962
                'to':   1143 ->   1143
                 '_':  30962 ->  30962
             'check':   7170 ->   7170
                 '}':  30983 ->  30983
                 '年':  54540 ->  54540
                 '，':  31123 ->  31123
                 '{':  30982 ->  30982
              'city':  17654 ->  17654
                 '_':  30962 ->  30962
                 '2':  30943 ->  30943
                 '}':  30983 ->  30983
                 '的':  54530 ->  54530
                '民用':  41801 ->  41801
                '汽车':  32031 ->  32031
                '拥有':  32104 ->  32104
                '量为':  44368 ->  44368
                 '：':  31211 ->  31211
                 '{':  30982 ->  30982
               'car':   6747 ->   6747
                 '_':  30962 ->  30962
             'count':   5597 ->   5597
                 '_':  30962 ->  30962
              'city':  17654 ->  17654
                 '_':  30962 ->  30962
                 '2':  30943 ->  30943
                 '}':  30983 ->  30983
                 '万':  38981 ->  38981
                 '辆':  55793 ->  55793
                '")':   2479 ->   2479
                  '':      2 ->      2
<<<<<<<<<<<<< Sanity Check
01/13/2024 20:42:29 - WARNING - accelerate.utils.other - Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:519] 2024-01-13 20:42:33,879 >> max_steps is given, it will override any value given in num_train_epochs
[WARNING|modeling_utils.py:2045] 2024-01-13 20:42:33,881 >> You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[INFO|trainer.py:1706] 2024-01-13 20:42:34,406 >> ***** Running training *****
[INFO|trainer.py:1707] 2024-01-13 20:42:34,407 >>   Num examples = 153
[INFO|trainer.py:1708] 2024-01-13 20:42:34,407 >>   Num Epochs = 34
[INFO|trainer.py:1709] 2024-01-13 20:42:34,407 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1712] 2024-01-13 20:42:34,407 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1713] 2024-01-13 20:42:34,407 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1714] 2024-01-13 20:42:34,407 >>   Total optimization steps = 300
[INFO|trainer.py:1715] 2024-01-13 20:42:34,408 >>   Number of trainable parameters = 1,835,008
  0%|          | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/300 [00:23<1:58:06, 23.70s/it]                                                 {'loss': 1.3498, 'learning_rate': 0.0049833333333333335, 'epoch': 0.1}
  0%|          | 1/300 [00:23<1:58:06, 23.70s/it]  1%|          | 2/300 [00:46<1:54:47, 23.11s/it]                                                 {'loss': 1.2632, 'learning_rate': 0.004966666666666667, 'epoch': 0.21}
  1%|          | 2/300 [00:46<1:54:47, 23.11s/it]  1%|          | 3/300 [01:09<1:53:42, 22.97s/it]                                                 {'loss': 1.284, 'learning_rate': 0.00495, 'epoch': 0.31}
  1%|          | 3/300 [01:09<1:53:42, 22.97s/it]  1%|▏         | 4/300 [01:32<1:53:04, 22.92s/it]                                                 {'loss': 1.4881, 'learning_rate': 0.004933333333333334, 'epoch': 0.42}
  1%|▏         | 4/300 [01:32<1:53:04, 22.92s/it]  2%|▏         | 5/300 [01:54<1:52:34, 22.90s/it]                                                 {'loss': 1.2036, 'learning_rate': 0.004916666666666666, 'epoch': 0.52}
  2%|▏         | 5/300 [01:54<1:52:34, 22.90s/it]  2%|▏         | 6/300 [02:17<1:52:12, 22.90s/it]                                                 {'loss': 1.249, 'learning_rate': 0.0049, 'epoch': 0.63}
  2%|▏         | 6/300 [02:17<1:52:12, 22.90s/it]  2%|▏         | 7/300 [02:40<1:51:51, 22.90s/it]                                                 {'loss': 1.226, 'learning_rate': 0.004883333333333333, 'epoch': 0.73}
  2%|▏         | 7/300 [02:40<1:51:51, 22.90s/it]  3%|▎         | 8/300 [03:03<1:51:28, 22.91s/it]                                                 {'loss': 1.1685, 'learning_rate': 0.004866666666666667, 'epoch': 0.84}
  3%|▎         | 8/300 [03:03<1:51:28, 22.91s/it]  3%|▎         | 9/300 [03:26<1:51:08, 22.92s/it]                                                 {'loss': 1.1146, 'learning_rate': 0.00485, 'epoch': 0.94}
  3%|▎         | 9/300 [03:26<1:51:08, 22.92s/it]  3%|▎         | 10/300 [03:49<1:50:46, 22.92s/it]                                                  {'loss': 1.1928, 'learning_rate': 0.004833333333333334, 'epoch': 1.05}
  3%|▎         | 10/300 [03:49<1:50:46, 22.92s/it]  4%|▎         | 11/300 [04:12<1:50:25, 22.93s/it]                                                  {'loss': 1.142, 'learning_rate': 0.004816666666666667, 'epoch': 1.15}
  4%|▎         | 11/300 [04:12<1:50:25, 22.93s/it]  4%|▍         | 12/300 [04:35<1:50:03, 22.93s/it]                                                  {'loss': 1.1467, 'learning_rate': 0.0048, 'epoch': 1.25}
  4%|▍         | 12/300 [04:35<1:50:03, 22.93s/it]  4%|▍         | 13/300 [04:58<1:49:40, 22.93s/it]                                                  {'loss': 1.1651, 'learning_rate': 0.004783333333333333, 'epoch': 1.36}
  4%|▍         | 13/300 [04:58<1:49:40, 22.93s/it]  5%|▍         | 14/300 [05:21<1:49:18, 22.93s/it]                                                  {'loss': 1.0461, 'learning_rate': 0.004766666666666667, 'epoch': 1.46}
  5%|▍         | 14/300 [05:21<1:49:18, 22.93s/it]  5%|▌         | 15/300 [05:44<1:48:53, 22.93s/it]                                                  {'loss': 1.0806, 'learning_rate': 0.00475, 'epoch': 1.57}
  5%|▌         | 15/300 [05:44<1:48:53, 22.93s/it]  5%|▌         | 16/300 [06:07<1:48:30, 22.92s/it]                                                  {'loss': 1.0578, 'learning_rate': 0.004733333333333333, 'epoch': 1.67}
  5%|▌         | 16/300 [06:07<1:48:30, 22.92s/it]  6%|▌         | 17/300 [06:29<1:48:05, 22.92s/it]                                                  {'loss': 1.0612, 'learning_rate': 0.004716666666666667, 'epoch': 1.78}
  6%|▌         | 17/300 [06:29<1:48:05, 22.92s/it]  6%|▌         | 18/300 [06:52<1:47:43, 22.92s/it]                                                  {'loss': 0.9905, 'learning_rate': 0.0047, 'epoch': 1.88}
  6%|▌         | 18/300 [06:52<1:47:43, 22.92s/it]  6%|▋         | 19/300 [07:15<1:47:22, 22.93s/it]                                                  {'loss': 0.9356, 'learning_rate': 0.004683333333333334, 'epoch': 1.99}
  6%|▋         | 19/300 [07:15<1:47:22, 22.93s/it]  7%|▋         | 20/300 [07:38<1:47:00, 22.93s/it]                                                  {'loss': 1.0011, 'learning_rate': 0.004666666666666667, 'epoch': 2.09}
  7%|▋         | 20/300 [07:38<1:47:00, 22.93s/it]  7%|▋         | 21/300 [08:01<1:46:37, 22.93s/it]                                                  {'loss': 1.0291, 'learning_rate': 0.0046500000000000005, 'epoch': 2.2}
  7%|▋         | 21/300 [08:01<1:46:37, 22.93s/it]  7%|▋         | 22/300 [08:24<1:46:12, 22.92s/it]                                                  {'loss': 1.0439, 'learning_rate': 0.004633333333333333, 'epoch': 2.3}
  7%|▋         | 22/300 [08:24<1:46:12, 22.92s/it]  8%|▊         | 23/300 [08:47<1:45:49, 22.92s/it]                                                  {'loss': 0.8775, 'learning_rate': 0.0046166666666666665, 'epoch': 2.41}
  8%|▊         | 23/300 [08:47<1:45:49, 22.92s/it]  8%|▊         | 24/300 [09:10<1:45:27, 22.93s/it]                                                  {'loss': 0.9145, 'learning_rate': 0.0046, 'epoch': 2.51}
  8%|▊         | 24/300 [09:10<1:45:27, 22.93s/it]  8%|▊         | 25/300 [09:33<1:45:08, 22.94s/it]                                                  {'loss': 0.9787, 'learning_rate': 0.004583333333333333, 'epoch': 2.61}
  8%|▊         | 25/300 [09:33<1:45:08, 22.94s/it]  9%|▊         | 26/300 [09:56<1:44:46, 22.94s/it]                                                  {'loss': 0.9337, 'learning_rate': 0.004566666666666667, 'epoch': 2.72}
  9%|▊         | 26/300 [09:56<1:44:46, 22.94s/it]  9%|▉         | 27/300 [10:19<1:44:22, 22.94s/it]                                                  {'loss': 0.8742, 'learning_rate': 0.00455, 'epoch': 2.82}
  9%|▉         | 27/300 [10:19<1:44:22, 22.94s/it]  9%|▉         | 28/300 [10:42<1:43:59, 22.94s/it]                                                  {'loss': 0.7548, 'learning_rate': 0.004533333333333333, 'epoch': 2.93}
  9%|▉         | 28/300 [10:42<1:43:59, 22.94s/it] 10%|▉         | 29/300 [11:05<1:43:35, 22.94s/it]                                                  {'loss': 0.9108, 'learning_rate': 0.004516666666666667, 'epoch': 3.03}
 10%|▉         | 29/300 [11:05<1:43:35, 22.94s/it] 10%|█         | 30/300 [11:28<1:43:13, 22.94s/it]                                                  {'loss': 0.9057, 'learning_rate': 0.0045000000000000005, 'epoch': 3.14}
 10%|█         | 30/300 [11:28<1:43:13, 22.94s/it] 10%|█         | 31/300 [11:51<1:42:50, 22.94s/it]                                                  {'loss': 0.8948, 'learning_rate': 0.004483333333333333, 'epoch': 3.24}
 10%|█         | 31/300 [11:51<1:42:50, 22.94s/it] 11%|█         | 32/300 [12:14<1:42:26, 22.94s/it]                                                  {'loss': 0.8635, 'learning_rate': 0.0044666666666666665, 'epoch': 3.35}
 11%|█         | 32/300 [12:14<1:42:26, 22.94s/it] 11%|█         | 33/300 [12:36<1:42:02, 22.93s/it]                                                  {'loss': 0.8345, 'learning_rate': 0.00445, 'epoch': 3.45}
 11%|█         | 33/300 [12:36<1:42:02, 22.93s/it] 11%|█▏        | 34/300 [12:59<1:41:40, 22.93s/it]                                                  {'loss': 0.9369, 'learning_rate': 0.004433333333333333, 'epoch': 3.56}
 11%|█▏        | 34/300 [12:59<1:41:40, 22.93s/it] 12%|█▏        | 35/300 [13:22<1:41:18, 22.94s/it]                                                  {'loss': 0.8235, 'learning_rate': 0.004416666666666667, 'epoch': 3.66}
 12%|█▏        | 35/300 [13:22<1:41:18, 22.94s/it] 12%|█▏        | 36/300 [13:45<1:40:55, 22.94s/it]                                                  {'loss': 0.868, 'learning_rate': 0.0044, 'epoch': 3.76}
 12%|█▏        | 36/300 [13:45<1:40:55, 22.94s/it] 12%|█▏        | 37/300 [14:08<1:40:32, 22.94s/it]                                                  {'loss': 0.8255, 'learning_rate': 0.004383333333333334, 'epoch': 3.87}
 12%|█▏        | 37/300 [14:08<1:40:32, 22.94s/it] 13%|█▎        | 38/300 [14:31<1:40:11, 22.94s/it]                                                  {'loss': 0.7815, 'learning_rate': 0.004366666666666666, 'epoch': 3.97}
 13%|█▎        | 38/300 [14:31<1:40:11, 22.94s/it] 13%|█▎        | 39/300 [14:54<1:39:46, 22.94s/it]                                                  {'loss': 0.942, 'learning_rate': 0.00435, 'epoch': 4.08}
 13%|█▎        | 39/300 [14:54<1:39:46, 22.94s/it] 13%|█▎        | 40/300 [15:17<1:39:24, 22.94s/it]                                                  {'loss': 0.762, 'learning_rate': 0.004333333333333334, 'epoch': 4.18}
 13%|█▎        | 40/300 [15:17<1:39:24, 22.94s/it] 14%|█▎        | 41/300 [15:40<1:39:03, 22.95s/it]                                                  {'loss': 0.7711, 'learning_rate': 0.004316666666666667, 'epoch': 4.29}
 14%|█▎        | 41/300 [15:40<1:39:03, 22.95s/it] 14%|█▍        | 42/300 [16:03<1:38:40, 22.95s/it]                                                  {'loss': 0.8073, 'learning_rate': 0.0043, 'epoch': 4.39}
 14%|█▍        | 42/300 [16:03<1:38:40, 22.95s/it] 14%|█▍        | 43/300 [16:26<1:38:15, 22.94s/it]                                                  {'loss': 0.8224, 'learning_rate': 0.0042833333333333334, 'epoch': 4.5}
 14%|█▍        | 43/300 [16:26<1:38:15, 22.94s/it] 15%|█▍        | 44/300 [16:49<1:37:51, 22.93s/it]                                                  {'loss': 0.9046, 'learning_rate': 0.004266666666666667, 'epoch': 4.6}
 15%|█▍        | 44/300 [16:49<1:37:51, 22.93s/it] 15%|█▌        | 45/300 [17:12<1:37:27, 22.93s/it]                                                  {'loss': 0.7575, 'learning_rate': 0.00425, 'epoch': 4.71}
 15%|█▌        | 45/300 [17:12<1:37:27, 22.93s/it] 15%|█▌        | 46/300 [17:35<1:37:02, 22.92s/it]                                                  {'loss': 0.8224, 'learning_rate': 0.004233333333333334, 'epoch': 4.81}
 15%|█▌        | 46/300 [17:35<1:37:02, 22.92s/it] 16%|█▌        | 47/300 [17:58<1:36:39, 22.92s/it]                                                  {'loss': 0.8501, 'learning_rate': 0.004216666666666667, 'epoch': 4.92}
 16%|█▌        | 47/300 [17:58<1:36:39, 22.92s/it] 16%|█▌        | 48/300 [18:20<1:36:19, 22.93s/it]                                                  {'loss': 0.8893, 'learning_rate': 0.0042, 'epoch': 5.02}
 16%|█▌        | 48/300 [18:20<1:36:19, 22.93s/it] 16%|█▋        | 49/300 [18:43<1:35:54, 22.93s/it]                                                  {'loss': 0.8868, 'learning_rate': 0.004183333333333333, 'epoch': 5.12}
 16%|█▋        | 49/300 [18:43<1:35:54, 22.93s/it] 17%|█▋        | 50/300 [19:06<1:35:31, 22.92s/it]                                                  {'loss': 0.7872, 'learning_rate': 0.004166666666666667, 'epoch': 5.23}
 17%|█▋        | 50/300 [19:06<1:35:31, 22.92s/it] 17%|█▋        | 51/300 [19:29<1:35:09, 22.93s/it]                                                  {'loss': 0.8085, 'learning_rate': 0.00415, 'epoch': 5.33}
 17%|█▋        | 51/300 [19:29<1:35:09, 22.93s/it] 17%|█▋        | 52/300 [19:52<1:34:47, 22.94s/it]                                                  {'loss': 0.8097, 'learning_rate': 0.0041333333333333335, 'epoch': 5.44}
 17%|█▋        | 52/300 [19:52<1:34:47, 22.94s/it] 18%|█▊        | 53/300 [20:15<1:34:26, 22.94s/it]                                                  {'loss': 0.7473, 'learning_rate': 0.004116666666666667, 'epoch': 5.54}
 18%|█▊        | 53/300 [20:15<1:34:26, 22.94s/it] 18%|█▊        | 54/300 [20:38<1:34:03, 22.94s/it]                                                  {'loss': 0.7458, 'learning_rate': 0.0040999999999999995, 'epoch': 5.65}
 18%|█▊        | 54/300 [20:38<1:34:03, 22.94s/it] 18%|█▊        | 55/300 [21:01<1:33:39, 22.94s/it]                                                  {'loss': 0.8248, 'learning_rate': 0.004083333333333333, 'epoch': 5.75}
 18%|█▊        | 55/300 [21:01<1:33:39, 22.94s/it] 19%|█▊        | 56/300 [21:24<1:33:19, 22.95s/it]                                                  {'loss': 0.7702, 'learning_rate': 0.004066666666666667, 'epoch': 5.86}
 19%|█▊        | 56/300 [21:24<1:33:19, 22.95s/it] 19%|█▉        | 57/300 [21:47<1:32:55, 22.94s/it]                                                  {'loss': 0.7664, 'learning_rate': 0.004050000000000001, 'epoch': 5.96}
 19%|█▉        | 57/300 [21:47<1:32:55, 22.94s/it] 19%|█▉        | 58/300 [22:10<1:32:31, 22.94s/it]                                                  {'loss': 0.7221, 'learning_rate': 0.004033333333333333, 'epoch': 6.07}
 19%|█▉        | 58/300 [22:10<1:32:31, 22.94s/it] 20%|█▉        | 59/300 [22:33<1:32:08, 22.94s/it]                                                  {'loss': 0.7604, 'learning_rate': 0.004016666666666667, 'epoch': 6.17}
 20%|█▉        | 59/300 [22:33<1:32:08, 22.94s/it] 20%|██        | 60/300 [22:56<1:31:46, 22.94s/it]                                                  {'loss': 0.7691, 'learning_rate': 0.004, 'epoch': 6.27}
 20%|██        | 60/300 [22:56<1:31:46, 22.94s/it] 20%|██        | 61/300 [23:19<1:31:22, 22.94s/it]                                                  {'loss': 0.7642, 'learning_rate': 0.0039833333333333335, 'epoch': 6.38}
 20%|██        | 61/300 [23:19<1:31:22, 22.94s/it] 21%|██        | 62/300 [23:42<1:30:58, 22.93s/it]                                                  {'loss': 0.7244, 'learning_rate': 0.003966666666666667, 'epoch': 6.48}
 21%|██        | 62/300 [23:42<1:30:58, 22.93s/it] 21%|██        | 63/300 [24:05<1:30:34, 22.93s/it]                                                  {'loss': 0.8329, 'learning_rate': 0.00395, 'epoch': 6.59}
 21%|██        | 63/300 [24:05<1:30:34, 22.93s/it] 21%|██▏       | 64/300 [24:27<1:30:13, 22.94s/it]                                                  {'loss': 0.7297, 'learning_rate': 0.003933333333333333, 'epoch': 6.69}
 21%|██▏       | 64/300 [24:27<1:30:13, 22.94s/it] 22%|██▏       | 65/300 [24:50<1:29:49, 22.94s/it]                                                  {'loss': 0.7081, 'learning_rate': 0.003916666666666666, 'epoch': 6.8}
 22%|██▏       | 65/300 [24:50<1:29:49, 22.94s/it] 22%|██▏       | 66/300 [25:13<1:29:24, 22.93s/it]                                                  {'loss': 0.7736, 'learning_rate': 0.0039000000000000003, 'epoch': 6.9}
 22%|██▏       | 66/300 [25:13<1:29:24, 22.93s/it] 22%|██▏       | 67/300 [25:36<1:29:02, 22.93s/it]                                                  {'loss': 0.7005, 'learning_rate': 0.0038833333333333333, 'epoch': 7.01}
 22%|██▏       | 67/300 [25:36<1:29:02, 22.93s/it] 23%|██▎       | 68/300 [25:59<1:28:42, 22.94s/it]                                                  {'loss': 0.746, 'learning_rate': 0.0038666666666666667, 'epoch': 7.11}
 23%|██▎       | 68/300 [25:59<1:28:42, 22.94s/it] 23%|██▎       | 69/300 [26:22<1:28:18, 22.94s/it]                                                  {'loss': 0.7339, 'learning_rate': 0.00385, 'epoch': 7.22}
 23%|██▎       | 69/300 [26:22<1:28:18, 22.94s/it] 23%|██▎       | 70/300 [26:45<1:27:53, 22.93s/it]                                                  {'loss': 0.6905, 'learning_rate': 0.0038333333333333336, 'epoch': 7.32}
 23%|██▎       | 70/300 [26:45<1:27:53, 22.93s/it] 24%|██▎       | 71/300 [27:08<1:27:30, 22.93s/it]                                                  {'loss': 0.7096, 'learning_rate': 0.0038166666666666666, 'epoch': 7.42}
 24%|██▎       | 71/300 [27:08<1:27:30, 22.93s/it] 24%|██▍       | 72/300 [27:31<1:27:08, 22.93s/it]                                                  {'loss': 0.7293, 'learning_rate': 0.0038, 'epoch': 7.53}
 24%|██▍       | 72/300 [27:31<1:27:08, 22.93s/it] 24%|██▍       | 73/300 [27:54<1:26:45, 22.93s/it]                                                  {'loss': 0.727, 'learning_rate': 0.0037833333333333334, 'epoch': 7.63}
 24%|██▍       | 73/300 [27:54<1:26:45, 22.93s/it] 25%|██▍       | 74/300 [28:17<1:26:22, 22.93s/it]                                                  {'loss': 0.7266, 'learning_rate': 0.0037666666666666664, 'epoch': 7.74}
 25%|██▍       | 74/300 [28:17<1:26:22, 22.93s/it] 25%|██▌       | 75/300 [28:40<1:26:01, 22.94s/it]                                                  {'loss': 0.6892, 'learning_rate': 0.00375, 'epoch': 7.84}
 25%|██▌       | 75/300 [28:40<1:26:01, 22.94s/it] 25%|██▌       | 76/300 [29:03<1:25:39, 22.95s/it]                                                  {'loss': 0.693, 'learning_rate': 0.0037333333333333337, 'epoch': 7.95}
 25%|██▌       | 76/300 [29:03<1:25:39, 22.95s/it] 26%|██▌       | 77/300 [29:26<1:25:17, 22.95s/it]                                                  {'loss': 0.6958, 'learning_rate': 0.0037166666666666667, 'epoch': 8.05}
 26%|██▌       | 77/300 [29:26<1:25:17, 22.95s/it] 26%|██▌       | 78/300 [29:49<1:24:53, 22.95s/it]                                                  {'loss': 0.6999, 'learning_rate': 0.0037, 'epoch': 8.16}
 26%|██▌       | 78/300 [29:49<1:24:53, 22.95s/it] 26%|██▋       | 79/300 [30:12<1:24:30, 22.94s/it]                                                  {'loss': 0.6462, 'learning_rate': 0.0036833333333333336, 'epoch': 8.26}
 26%|██▋       | 79/300 [30:12<1:24:30, 22.94s/it] 27%|██▋       | 80/300 [30:34<1:24:08, 22.95s/it]                                                  {'loss': 0.7244, 'learning_rate': 0.0036666666666666666, 'epoch': 8.37}
 27%|██▋       | 80/300 [30:34<1:24:08, 22.95s/it] 27%|██▋       | 81/300 [30:57<1:23:45, 22.95s/it]                                                  {'loss': 0.6448, 'learning_rate': 0.00365, 'epoch': 8.47}
 27%|██▋       | 81/300 [30:57<1:23:45, 22.95s/it] 27%|██▋       | 82/300 [31:20<1:23:22, 22.95s/it]                                                  {'loss': 0.7145, 'learning_rate': 0.0036333333333333335, 'epoch': 8.58}
 27%|██▋       | 82/300 [31:20<1:23:22, 22.95s/it] 28%|██▊       | 83/300 [31:43<1:22:59, 22.95s/it]                                                  {'loss': 0.722, 'learning_rate': 0.003616666666666667, 'epoch': 8.68}
 28%|██▊       | 83/300 [31:43<1:22:59, 22.95s/it] 28%|██▊       | 84/300 [32:06<1:22:37, 22.95s/it]                                                  {'loss': 0.6481, 'learning_rate': 0.0036, 'epoch': 8.78}
 28%|██▊       | 84/300 [32:06<1:22:37, 22.95s/it] 28%|██▊       | 85/300 [32:29<1:22:14, 22.95s/it]                                                  {'loss': 0.7165, 'learning_rate': 0.0035833333333333333, 'epoch': 8.89}
 28%|██▊       | 85/300 [32:29<1:22:14, 22.95s/it] 29%|██▊       | 86/300 [32:52<1:21:51, 22.95s/it]                                                  {'loss': 0.6925, 'learning_rate': 0.0035666666666666668, 'epoch': 8.99}
 29%|██▊       | 86/300 [32:52<1:21:51, 22.95s/it] 29%|██▉       | 87/300 [33:15<1:21:26, 22.94s/it]                                                  {'loss': 0.6895, 'learning_rate': 0.0035499999999999998, 'epoch': 9.1}
 29%|██▉       | 87/300 [33:15<1:21:26, 22.94s/it] 29%|██▉       | 88/300 [33:38<1:21:03, 22.94s/it]                                                  {'loss': 0.5779, 'learning_rate': 0.003533333333333333, 'epoch': 9.2}
 29%|██▉       | 88/300 [33:38<1:21:03, 22.94s/it] 30%|██▉       | 89/300 [34:01<1:20:42, 22.95s/it]                                                  {'loss': 0.6477, 'learning_rate': 0.003516666666666667, 'epoch': 9.31}
 30%|██▉       | 89/300 [34:01<1:20:42, 22.95s/it] 30%|███       | 90/300 [34:24<1:20:18, 22.94s/it]                                                  {'loss': 0.6988, 'learning_rate': 0.0034999999999999996, 'epoch': 9.41}
 30%|███       | 90/300 [34:24<1:20:18, 22.94s/it] 30%|███       | 91/300 [34:47<1:19:56, 22.95s/it]                                                  {'loss': 0.68, 'learning_rate': 0.0034833333333333335, 'epoch': 9.52}
 30%|███       | 91/300 [34:47<1:19:56, 22.95s/it] 31%|███       | 92/300 [35:10<1:19:32, 22.95s/it]                                                  {'loss': 0.6556, 'learning_rate': 0.003466666666666667, 'epoch': 9.62}
 31%|███       | 92/300 [35:10<1:19:32, 22.95s/it] 31%|███       | 93/300 [35:33<1:19:09, 22.95s/it]                                                  {'loss': 0.6962, 'learning_rate': 0.00345, 'epoch': 9.73}
 31%|███       | 93/300 [35:33<1:19:09, 22.95s/it] 31%|███▏      | 94/300 [35:56<1:18:48, 22.95s/it]                                                  {'loss': 0.5892, 'learning_rate': 0.0034333333333333334, 'epoch': 9.83}
 31%|███▏      | 94/300 [35:56<1:18:48, 22.95s/it] 32%|███▏      | 95/300 [36:19<1:18:24, 22.95s/it]                                                  {'loss': 0.6616, 'learning_rate': 0.003416666666666667, 'epoch': 9.93}
 32%|███▏      | 95/300 [36:19<1:18:24, 22.95s/it] 32%|███▏      | 96/300 [36:42<1:18:01, 22.95s/it]                                                  {'loss': 0.6775, 'learning_rate': 0.0034000000000000002, 'epoch': 10.04}
 32%|███▏      | 96/300 [36:42<1:18:01, 22.95s/it] 32%|███▏      | 97/300 [37:05<1:17:40, 22.96s/it]                                                  {'loss': 0.6327, 'learning_rate': 0.0033833333333333332, 'epoch': 10.14}
 32%|███▏      | 97/300 [37:05<1:17:40, 22.96s/it] 33%|███▎      | 98/300 [37:28<1:17:14, 22.94s/it]                                                  {'loss': 0.6482, 'learning_rate': 0.0033666666666666667, 'epoch': 10.25}
 33%|███▎      | 98/300 [37:28<1:17:14, 22.94s/it] 33%|███▎      | 99/300 [37:51<1:16:53, 22.95s/it]                                                  {'loss': 0.626, 'learning_rate': 0.00335, 'epoch': 10.35}
 33%|███▎      | 99/300 [37:51<1:16:53, 22.95s/it] 33%|███▎      | 100/300 [38:13<1:16:31, 22.96s/it]                                                   {'loss': 0.6084, 'learning_rate': 0.003333333333333333, 'epoch': 10.46}
 33%|███▎      | 100/300 [38:13<1:16:31, 22.96s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:483] 2024-01-13 21:20:48,404 >> Configuration saved in output/test-202401132034-128-5e-3/tmp-checkpoint-100/config.json
[INFO|configuration_utils.py:594] 2024-01-13 21:20:48,405 >> Configuration saved in output/test-202401132034-128-5e-3/tmp-checkpoint-100/generation_config.json
[INFO|modeling_utils.py:2382] 2024-01-13 21:20:48,416 >> Model weights saved in output/test-202401132034-128-5e-3/tmp-checkpoint-100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2432] 2024-01-13 21:20:48,416 >> tokenizer config file saved in output/test-202401132034-128-5e-3/tmp-checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-01-13 21:20:48,417 >> Special tokens file saved in output/test-202401132034-128-5e-3/tmp-checkpoint-100/special_tokens_map.json
/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 34%|███▎      | 101/300 [38:36<1:16:10, 22.97s/it]                                                   {'loss': 0.5853, 'learning_rate': 0.0033166666666666665, 'epoch': 10.56}
 34%|███▎      | 101/300 [38:36<1:16:10, 22.97s/it] 34%|███▍      | 102/300 [38:59<1:15:45, 22.96s/it]                                                   {'loss': 0.6402, 'learning_rate': 0.0033000000000000004, 'epoch': 10.67}
 34%|███▍      | 102/300 [38:59<1:15:45, 22.96s/it] 34%|███▍      | 103/300 [39:22<1:15:23, 22.96s/it]                                                   {'loss': 0.5921, 'learning_rate': 0.003283333333333333, 'epoch': 10.77}
 34%|███▍      | 103/300 [39:22<1:15:23, 22.96s/it] 35%|███▍      | 104/300 [39:45<1:14:59, 22.96s/it]                                                   {'loss': 0.5761, 'learning_rate': 0.003266666666666667, 'epoch': 10.88}
 35%|███▍      | 104/300 [39:45<1:14:59, 22.96s/it] 35%|███▌      | 105/300 [40:08<1:14:36, 22.95s/it]                                                   {'loss': 0.7149, 'learning_rate': 0.0032500000000000003, 'epoch': 10.98}
 35%|███▌      | 105/300 [40:08<1:14:36, 22.95s/it] 35%|███▌      | 106/300 [40:31<1:14:14, 22.96s/it]                                                   {'loss': 0.6219, 'learning_rate': 0.0032333333333333333, 'epoch': 11.08}
 35%|███▌      | 106/300 [40:31<1:14:14, 22.96s/it] 36%|███▌      | 107/300 [40:54<1:13:49, 22.95s/it]                                                   {'loss': 0.6008, 'learning_rate': 0.0032166666666666667, 'epoch': 11.19}
 36%|███▌      | 107/300 [40:54<1:13:49, 22.95s/it] 36%|███▌      | 108/300 [41:17<1:13:24, 22.94s/it]                                                   {'loss': 0.5774, 'learning_rate': 0.0032, 'epoch': 11.29}
 36%|███▌      | 108/300 [41:17<1:13:24, 22.94s/it] 36%|███▋      | 109/300 [41:40<1:13:01, 22.94s/it]                                                   {'loss': 0.5954, 'learning_rate': 0.0031833333333333336, 'epoch': 11.4}
 36%|███▋      | 109/300 [41:40<1:13:01, 22.94s/it] 37%|███▋      | 110/300 [42:03<1:12:39, 22.94s/it]                                                   {'loss': 0.5894, 'learning_rate': 0.0031666666666666666, 'epoch': 11.5}
 37%|███▋      | 110/300 [42:03<1:12:39, 22.94s/it] 37%|███▋      | 111/300 [42:26<1:12:16, 22.95s/it]                                                   {'loss': 0.6615, 'learning_rate': 0.00315, 'epoch': 11.61}
 37%|███▋      | 111/300 [42:26<1:12:16, 22.95s/it] 37%|███▋      | 112/300 [42:49<1:11:55, 22.95s/it]                                                   {'loss': 0.6559, 'learning_rate': 0.0031333333333333335, 'epoch': 11.71}
 37%|███▋      | 112/300 [42:49<1:11:55, 22.95s/it] 38%|███▊      | 113/300 [43:12<1:11:32, 22.96s/it]                                                   {'loss': 0.5328, 'learning_rate': 0.0031166666666666665, 'epoch': 11.82}
 38%|███▊      | 113/300 [43:12<1:11:32, 22.96s/it] 38%|███▊      | 114/300 [43:35<1:11:10, 22.96s/it]                                                   {'loss': 0.6521, 'learning_rate': 0.0031, 'epoch': 11.92}
 38%|███▊      | 114/300 [43:35<1:11:10, 22.96s/it] 38%|███▊      | 115/300 [43:58<1:10:47, 22.96s/it]                                                   {'loss': 0.5151, 'learning_rate': 0.0030833333333333338, 'epoch': 12.03}
 38%|███▊      | 115/300 [43:58<1:10:47, 22.96s/it] 39%|███▊      | 116/300 [44:21<1:10:23, 22.95s/it]                                                   {'loss': 0.5826, 'learning_rate': 0.0030666666666666663, 'epoch': 12.13}
 39%|███▊      | 116/300 [44:21<1:10:23, 22.95s/it] 39%|███▉      | 117/300 [44:44<1:10:01, 22.96s/it]                                                   {'loss': 0.6085, 'learning_rate': 0.00305, 'epoch': 12.24}
 39%|███▉      | 117/300 [44:44<1:10:01, 22.96s/it] 39%|███▉      | 118/300 [45:07<1:09:38, 22.96s/it]                                                   {'loss': 0.5974, 'learning_rate': 0.0030333333333333336, 'epoch': 12.34}
 39%|███▉      | 118/300 [45:07<1:09:38, 22.96s/it] 40%|███▉      | 119/300 [45:30<1:09:15, 22.96s/it]                                                   {'loss': 0.5915, 'learning_rate': 0.003016666666666667, 'epoch': 12.44}
 40%|███▉      | 119/300 [45:30<1:09:15, 22.96s/it] 40%|████      | 120/300 [45:53<1:08:52, 22.96s/it]                                                   {'loss': 0.5973, 'learning_rate': 0.003, 'epoch': 12.55}
 40%|████      | 120/300 [45:53<1:08:52, 22.96s/it] 40%|████      | 121/300 [46:16<1:08:28, 22.95s/it]                                                   {'loss': 0.6256, 'learning_rate': 0.0029833333333333335, 'epoch': 12.65}
 40%|████      | 121/300 [46:16<1:08:28, 22.95s/it] 41%|████      | 122/300 [46:38<1:08:04, 22.95s/it]                                                   {'loss': 0.5937, 'learning_rate': 0.002966666666666667, 'epoch': 12.76}
 41%|████      | 122/300 [46:38<1:08:04, 22.95s/it] 41%|████      | 123/300 [47:01<1:07:43, 22.96s/it]                                                   {'loss': 0.562, 'learning_rate': 0.00295, 'epoch': 12.86}
 41%|████      | 123/300 [47:01<1:07:43, 22.96s/it] 41%|████▏     | 124/300 [47:24<1:07:20, 22.96s/it]                                                   {'loss': 0.5462, 'learning_rate': 0.0029333333333333334, 'epoch': 12.97}
 41%|████▏     | 124/300 [47:24<1:07:20, 22.96s/it] 42%|████▏     | 125/300 [47:47<1:06:56, 22.95s/it]                                                   {'loss': 0.5413, 'learning_rate': 0.002916666666666667, 'epoch': 13.07}
 42%|████▏     | 125/300 [47:47<1:06:56, 22.95s/it] 42%|████▏     | 126/300 [48:10<1:06:34, 22.96s/it]                                                   {'loss': 0.5651, 'learning_rate': 0.0029, 'epoch': 13.18}
 42%|████▏     | 126/300 [48:10<1:06:34, 22.96s/it] 42%|████▏     | 127/300 [48:33<1:06:11, 22.96s/it]                                                   {'loss': 0.514, 'learning_rate': 0.0028833333333333332, 'epoch': 13.28}
 42%|████▏     | 127/300 [48:33<1:06:11, 22.96s/it] 43%|████▎     | 128/300 [48:56<1:05:49, 22.96s/it]                                                   {'loss': 0.6177, 'learning_rate': 0.0028666666666666667, 'epoch': 13.39}
 43%|████▎     | 128/300 [48:56<1:05:49, 22.96s/it] 43%|████▎     | 129/300 [49:19<1:05:26, 22.96s/it]                                                   {'loss': 0.5843, 'learning_rate': 0.0028499999999999997, 'epoch': 13.49}
 43%|████▎     | 129/300 [49:19<1:05:26, 22.96s/it] 43%|████▎     | 130/300 [49:42<1:05:03, 22.96s/it]                                                   {'loss': 0.4979, 'learning_rate': 0.002833333333333333, 'epoch': 13.59}
 43%|████▎     | 130/300 [49:42<1:05:03, 22.96s/it] 44%|████▎     | 131/300 [50:05<1:04:40, 22.96s/it]                                                   {'loss': 0.6346, 'learning_rate': 0.002816666666666667, 'epoch': 13.7}
 44%|████▎     | 131/300 [50:05<1:04:40, 22.96s/it] 44%|████▍     | 132/300 [50:28<1:04:18, 22.97s/it]                                                   {'loss': 0.5867, 'learning_rate': 0.0028000000000000004, 'epoch': 13.8}
 44%|████▍     | 132/300 [50:28<1:04:18, 22.97s/it] 44%|████▍     | 133/300 [50:51<1:03:54, 22.96s/it]                                                   {'loss': 0.6133, 'learning_rate': 0.0027833333333333334, 'epoch': 13.91}
 44%|████▍     | 133/300 [50:51<1:03:54, 22.96s/it] 45%|████▍     | 134/300 [51:14<1:03:30, 22.95s/it]                                                   {'loss': 0.4939, 'learning_rate': 0.002766666666666667, 'epoch': 14.01}
 45%|████▍     | 134/300 [51:14<1:03:30, 22.95s/it] 45%|████▌     | 135/300 [51:37<1:03:07, 22.96s/it]                                                   {'loss': 0.5402, 'learning_rate': 0.0027500000000000003, 'epoch': 14.12}
 45%|████▌     | 135/300 [51:37<1:03:07, 22.96s/it] 45%|████▌     | 136/300 [52:00<1:02:43, 22.95s/it]                                                   {'loss': 0.5327, 'learning_rate': 0.0027333333333333333, 'epoch': 14.22}
 45%|████▌     | 136/300 [52:00<1:02:43, 22.95s/it] 46%|████▌     | 137/300 [52:23<1:02:20, 22.95s/it]                                                   {'loss': 0.6431, 'learning_rate': 0.0027166666666666667, 'epoch': 14.33}
 46%|████▌     | 137/300 [52:23<1:02:20, 22.95s/it] 46%|████▌     | 138/300 [52:46<1:01:59, 22.96s/it]                                                   {'loss': 0.6006, 'learning_rate': 0.0027, 'epoch': 14.43}
 46%|████▌     | 138/300 [52:46<1:01:59, 22.96s/it] 46%|████▋     | 139/300 [53:09<1:01:35, 22.96s/it]                                                   {'loss': 0.4995, 'learning_rate': 0.002683333333333333, 'epoch': 14.54}
 46%|████▋     | 139/300 [53:09<1:01:35, 22.96s/it] 47%|████▋     | 140/300 [53:32<1:01:13, 22.96s/it]                                                   {'loss': 0.6186, 'learning_rate': 0.0026666666666666666, 'epoch': 14.64}
 47%|████▋     | 140/300 [53:32<1:01:13, 22.96s/it] 47%|████▋     | 141/300 [53:55<1:00:52, 22.97s/it]                                                   {'loss': 0.5683, 'learning_rate': 0.00265, 'epoch': 14.75}
 47%|████▋     | 141/300 [53:55<1:00:52, 22.97s/it] 47%|████▋     | 142/300 [54:18<1:00:28, 22.96s/it]                                                   {'loss': 0.4698, 'learning_rate': 0.002633333333333333, 'epoch': 14.85}
 47%|████▋     | 142/300 [54:18<1:00:28, 22.96s/it] 48%|████▊     | 143/300 [54:41<1:00:04, 22.96s/it]                                                   {'loss': 0.5581, 'learning_rate': 0.0026166666666666664, 'epoch': 14.95}
 48%|████▊     | 143/300 [54:41<1:00:04, 22.96s/it] 48%|████▊     | 144/300 [55:04<59:41, 22.96s/it]                                                   {'loss': 0.4821, 'learning_rate': 0.0026000000000000003, 'epoch': 15.06}
 48%|████▊     | 144/300 [55:04<59:41, 22.96s/it] 48%|████▊     | 145/300 [55:27<59:18, 22.96s/it]                                                 {'loss': 0.5679, 'learning_rate': 0.0025833333333333337, 'epoch': 15.16}
 48%|████▊     | 145/300 [55:27<59:18, 22.96s/it] 49%|████▊     | 146/300 [55:50<58:55, 22.96s/it]                                                 {'loss': 0.5959, 'learning_rate': 0.0025666666666666667, 'epoch': 15.27}
 49%|████▊     | 146/300 [55:50<58:55, 22.96s/it] 49%|████▉     | 147/300 [56:12<58:32, 22.96s/it]                                                 {'loss': 0.501, 'learning_rate': 0.00255, 'epoch': 15.37}
 49%|████▉     | 147/300 [56:12<58:32, 22.96s/it] 49%|████▉     | 148/300 [56:35<58:10, 22.97s/it]                                                 {'loss': 0.5463, 'learning_rate': 0.0025333333333333336, 'epoch': 15.48}
 49%|████▉     | 148/300 [56:35<58:10, 22.97s/it] 50%|████▉     | 149/300 [56:58<57:46, 22.96s/it]                                                 {'loss': 0.5193, 'learning_rate': 0.0025166666666666666, 'epoch': 15.58}
 50%|████▉     | 149/300 [56:58<57:46, 22.96s/it] 50%|█████     | 150/300 [57:21<57:23, 22.95s/it]                                                 {'loss': 0.5517, 'learning_rate': 0.0025, 'epoch': 15.69}
 50%|█████     | 150/300 [57:21<57:23, 22.95s/it] 50%|█████     | 151/300 [57:44<56:58, 22.95s/it]                                                 {'loss': 0.5331, 'learning_rate': 0.0024833333333333335, 'epoch': 15.79}
 50%|█████     | 151/300 [57:44<56:58, 22.95s/it] 51%|█████     | 152/300 [58:07<56:37, 22.96s/it]                                                 {'loss': 0.5948, 'learning_rate': 0.002466666666666667, 'epoch': 15.9}
 51%|█████     | 152/300 [58:07<56:37, 22.96s/it] 51%|█████     | 153/300 [58:30<56:13, 22.95s/it]                                                 {'loss': 0.5092, 'learning_rate': 0.00245, 'epoch': 16.0}
 51%|█████     | 153/300 [58:30<56:13, 22.95s/it] 51%|█████▏    | 154/300 [58:53<55:51, 22.95s/it]                                                 {'loss': 0.4921, 'learning_rate': 0.0024333333333333334, 'epoch': 16.1}
 51%|█████▏    | 154/300 [58:53<55:51, 22.95s/it] 52%|█████▏    | 155/300 [59:16<55:27, 22.95s/it]                                                 {'loss': 0.5766, 'learning_rate': 0.002416666666666667, 'epoch': 16.21}
 52%|█████▏    | 155/300 [59:16<55:27, 22.95s/it] 52%|█████▏    | 156/300 [59:39<55:06, 22.96s/it]                                                 {'loss': 0.5737, 'learning_rate': 0.0024, 'epoch': 16.31}
 52%|█████▏    | 156/300 [59:39<55:06, 22.96s/it] 52%|█████▏    | 157/300 [1:00:02<54:43, 22.96s/it]                                                   {'loss': 0.6267, 'learning_rate': 0.0023833333333333337, 'epoch': 16.42}
 52%|█████▏    | 157/300 [1:00:02<54:43, 22.96s/it] 53%|█████▎    | 158/300 [1:00:25<54:22, 22.97s/it]                                                   {'loss': 0.5193, 'learning_rate': 0.0023666666666666667, 'epoch': 16.52}
 53%|█████▎    | 158/300 [1:00:25<54:22, 22.97s/it] 53%|█████▎    | 159/300 [1:00:48<53:58, 22.97s/it]                                                   {'loss': 0.4813, 'learning_rate': 0.00235, 'epoch': 16.63}
 53%|█████▎    | 159/300 [1:00:48<53:58, 22.97s/it] 53%|█████▎    | 160/300 [1:01:11<53:34, 22.96s/it]                                                   {'loss': 0.4862, 'learning_rate': 0.0023333333333333335, 'epoch': 16.73}
 53%|█████▎    | 160/300 [1:01:11<53:34, 22.96s/it] 54%|█████▎    | 161/300 [1:01:34<53:11, 22.96s/it]                                                   {'loss': 0.5224, 'learning_rate': 0.0023166666666666665, 'epoch': 16.84}
 54%|█████▎    | 161/300 [1:01:34<53:11, 22.96s/it] 54%|█████▍    | 162/300 [1:01:57<52:48, 22.96s/it]                                                   {'loss': 0.547, 'learning_rate': 0.0023, 'epoch': 16.94}
 54%|█████▍    | 162/300 [1:01:57<52:48, 22.96s/it] 54%|█████▍    | 163/300 [1:02:20<52:25, 22.96s/it]                                                   {'loss': 0.5337, 'learning_rate': 0.0022833333333333334, 'epoch': 17.05}
 54%|█████▍    | 163/300 [1:02:20<52:25, 22.96s/it] 55%|█████▍    | 164/300 [1:02:43<52:02, 22.96s/it]                                                   {'loss': 0.493, 'learning_rate': 0.0022666666666666664, 'epoch': 17.15}
 55%|█████▍    | 164/300 [1:02:43<52:02, 22.96s/it] 55%|█████▌    | 165/300 [1:03:06<51:39, 22.96s/it]                                                   {'loss': 0.5741, 'learning_rate': 0.0022500000000000003, 'epoch': 17.25}
 55%|█████▌    | 165/300 [1:03:06<51:39, 22.96s/it] 55%|█████▌    | 166/300 [1:03:29<51:17, 22.96s/it]                                                   {'loss': 0.4789, 'learning_rate': 0.0022333333333333333, 'epoch': 17.36}
 55%|█████▌    | 166/300 [1:03:29<51:17, 22.96s/it] 56%|█████▌    | 167/300 [1:03:52<50:55, 22.97s/it]                                                   {'loss': 0.5704, 'learning_rate': 0.0022166666666666667, 'epoch': 17.46}
 56%|█████▌    | 167/300 [1:03:52<50:55, 22.97s/it] 56%|█████▌    | 168/300 [1:04:15<50:31, 22.97s/it]                                                   {'loss': 0.5176, 'learning_rate': 0.0022, 'epoch': 17.57}
 56%|█████▌    | 168/300 [1:04:15<50:31, 22.97s/it] 56%|█████▋    | 169/300 [1:04:38<50:08, 22.96s/it]                                                   {'loss': 0.5494, 'learning_rate': 0.002183333333333333, 'epoch': 17.67}
 56%|█████▋    | 169/300 [1:04:38<50:08, 22.96s/it] 57%|█████▋    | 170/300 [1:05:01<49:45, 22.96s/it]                                                   {'loss': 0.534, 'learning_rate': 0.002166666666666667, 'epoch': 17.78}
 57%|█████▋    | 170/300 [1:05:01<49:45, 22.96s/it] 57%|█████▋    | 171/300 [1:05:24<49:21, 22.96s/it]                                                   {'loss': 0.5419, 'learning_rate': 0.00215, 'epoch': 17.88}
 57%|█████▋    | 171/300 [1:05:24<49:21, 22.96s/it] 57%|█████▋    | 172/300 [1:05:46<48:59, 22.96s/it]                                                   {'loss': 0.4563, 'learning_rate': 0.0021333333333333334, 'epoch': 17.99}
 57%|█████▋    | 172/300 [1:05:46<48:59, 22.96s/it] 58%|█████▊    | 173/300 [1:06:09<48:37, 22.97s/it]                                                   {'loss': 0.4774, 'learning_rate': 0.002116666666666667, 'epoch': 18.09}
 58%|█████▊    | 173/300 [1:06:09<48:37, 22.97s/it] 58%|█████▊    | 174/300 [1:06:32<48:14, 22.97s/it]                                                   {'loss': 0.5107, 'learning_rate': 0.0021, 'epoch': 18.2}
 58%|█████▊    | 174/300 [1:06:32<48:14, 22.97s/it] 58%|█████▊    | 175/300 [1:06:55<47:50, 22.97s/it]                                                   {'loss': 0.487, 'learning_rate': 0.0020833333333333333, 'epoch': 18.3}
 58%|█████▊    | 175/300 [1:06:55<47:50, 22.97s/it] 59%|█████▊    | 176/300 [1:07:18<47:28, 22.97s/it]                                                   {'loss': 0.537, 'learning_rate': 0.0020666666666666667, 'epoch': 18.41}
 59%|█████▊    | 176/300 [1:07:18<47:28, 22.97s/it] 59%|█████▉    | 177/300 [1:07:41<47:05, 22.97s/it]                                                   {'loss': 0.4417, 'learning_rate': 0.0020499999999999997, 'epoch': 18.51}
 59%|█████▉    | 177/300 [1:07:41<47:05, 22.97s/it] 59%|█████▉    | 178/300 [1:08:04<46:41, 22.96s/it]                                                   {'loss': 0.5022, 'learning_rate': 0.0020333333333333336, 'epoch': 18.61}
 59%|█████▉    | 178/300 [1:08:04<46:41, 22.96s/it] 60%|█████▉    | 179/300 [1:08:27<46:18, 22.96s/it]                                                   {'loss': 0.5451, 'learning_rate': 0.0020166666666666666, 'epoch': 18.72}
 60%|█████▉    | 179/300 [1:08:27<46:18, 22.96s/it] 60%|██████    | 180/300 [1:08:50<45:56, 22.97s/it]                                                   {'loss': 0.5572, 'learning_rate': 0.002, 'epoch': 18.82}
 60%|██████    | 180/300 [1:08:50<45:56, 22.97s/it] 60%|██████    | 181/300 [1:09:13<45:33, 22.97s/it]                                                   {'loss': 0.5995, 'learning_rate': 0.0019833333333333335, 'epoch': 18.93}
 60%|██████    | 181/300 [1:09:13<45:33, 22.97s/it] 61%|██████    | 182/300 [1:09:36<45:09, 22.96s/it]                                                   {'loss': 0.5246, 'learning_rate': 0.0019666666666666665, 'epoch': 19.03}
 61%|██████    | 182/300 [1:09:36<45:09, 22.96s/it] 61%|██████    | 183/300 [1:09:59<44:46, 22.96s/it]                                                   {'loss': 0.5704, 'learning_rate': 0.0019500000000000001, 'epoch': 19.14}
 61%|██████    | 183/300 [1:09:59<44:46, 22.96s/it] 61%|██████▏   | 184/300 [1:10:22<44:22, 22.96s/it]                                                   {'loss': 0.4928, 'learning_rate': 0.0019333333333333333, 'epoch': 19.24}
 61%|██████▏   | 184/300 [1:10:22<44:22, 22.96s/it] 62%|██████▏   | 185/300 [1:10:45<44:00, 22.96s/it]                                                   {'loss': 0.5377, 'learning_rate': 0.0019166666666666668, 'epoch': 19.35}
 62%|██████▏   | 185/300 [1:10:45<44:00, 22.96s/it] 62%|██████▏   | 186/300 [1:11:08<43:37, 22.96s/it]                                                   {'loss': 0.5322, 'learning_rate': 0.0019, 'epoch': 19.45}
 62%|██████▏   | 186/300 [1:11:08<43:37, 22.96s/it] 62%|██████▏   | 187/300 [1:11:31<43:14, 22.96s/it]                                                   {'loss': 0.4494, 'learning_rate': 0.0018833333333333332, 'epoch': 19.56}
 62%|██████▏   | 187/300 [1:11:31<43:14, 22.96s/it] 63%|██████▎   | 188/300 [1:11:54<42:51, 22.96s/it]                                                   {'loss': 0.4937, 'learning_rate': 0.0018666666666666669, 'epoch': 19.66}
 63%|██████▎   | 188/300 [1:11:54<42:51, 22.96s/it] 63%|██████▎   | 189/300 [1:12:17<42:28, 22.96s/it]                                                   {'loss': 0.5183, 'learning_rate': 0.00185, 'epoch': 19.76}
 63%|██████▎   | 189/300 [1:12:17<42:28, 22.96s/it] 63%|██████▎   | 190/300 [1:12:40<42:06, 22.97s/it]                                                   {'loss': 0.4876, 'learning_rate': 0.0018333333333333333, 'epoch': 19.87}
 63%|██████▎   | 190/300 [1:12:40<42:06, 22.97s/it] 64%|██████▎   | 191/300 [1:13:03<41:43, 22.97s/it]                                                   {'loss': 0.505, 'learning_rate': 0.0018166666666666667, 'epoch': 19.97}
 64%|██████▎   | 191/300 [1:13:03<41:43, 22.97s/it] 64%|██████▍   | 192/300 [1:13:26<41:20, 22.97s/it]                                                   {'loss': 0.4802, 'learning_rate': 0.0018, 'epoch': 20.08}
 64%|██████▍   | 192/300 [1:13:26<41:20, 22.97s/it] 64%|██████▍   | 193/300 [1:13:49<40:56, 22.96s/it]                                                   {'loss': 0.4388, 'learning_rate': 0.0017833333333333334, 'epoch': 20.18}
 64%|██████▍   | 193/300 [1:13:49<40:56, 22.96s/it] 65%|██████▍   | 194/300 [1:14:12<40:32, 22.95s/it]                                                   {'loss': 0.5412, 'learning_rate': 0.0017666666666666666, 'epoch': 20.29}
 65%|██████▍   | 194/300 [1:14:12<40:32, 22.95s/it] 65%|██████▌   | 195/300 [1:14:35<40:09, 22.95s/it]                                                   {'loss': 0.5883, 'learning_rate': 0.0017499999999999998, 'epoch': 20.39}
 65%|██████▌   | 195/300 [1:14:35<40:09, 22.95s/it] 65%|██████▌   | 196/300 [1:14:58<39:47, 22.96s/it]                                                   {'loss': 0.5372, 'learning_rate': 0.0017333333333333335, 'epoch': 20.5}
 65%|██████▌   | 196/300 [1:14:58<39:47, 22.96s/it] 66%|██████▌   | 197/300 [1:15:21<39:25, 22.97s/it]                                                   {'loss': 0.5701, 'learning_rate': 0.0017166666666666667, 'epoch': 20.6}
 66%|██████▌   | 197/300 [1:15:21<39:25, 22.97s/it] 66%|██████▌   | 198/300 [1:15:44<39:02, 22.97s/it]                                                   {'loss': 0.4179, 'learning_rate': 0.0017000000000000001, 'epoch': 20.71}
 66%|██████▌   | 198/300 [1:15:44<39:02, 22.97s/it] 66%|██████▋   | 199/300 [1:16:07<38:39, 22.96s/it]                                                   {'loss': 0.4556, 'learning_rate': 0.0016833333333333333, 'epoch': 20.81}
 66%|██████▋   | 199/300 [1:16:07<38:39, 22.96s/it] 67%|██████▋   | 200/300 [1:16:29<38:16, 22.96s/it]                                                   {'loss': 0.4807, 'learning_rate': 0.0016666666666666666, 'epoch': 20.92}
 67%|██████▋   | 200/300 [1:16:29<38:16, 22.96s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:483] 2024-01-13 21:59:04,396 >> Configuration saved in output/test-202401132034-128-5e-3/tmp-checkpoint-200/config.json
[INFO|configuration_utils.py:594] 2024-01-13 21:59:04,396 >> Configuration saved in output/test-202401132034-128-5e-3/tmp-checkpoint-200/generation_config.json
[INFO|modeling_utils.py:2382] 2024-01-13 21:59:04,406 >> Model weights saved in output/test-202401132034-128-5e-3/tmp-checkpoint-200/pytorch_model.bin
[INFO|tokenization_utils_base.py:2432] 2024-01-13 21:59:04,407 >> tokenizer config file saved in output/test-202401132034-128-5e-3/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-01-13 21:59:04,407 >> Special tokens file saved in output/test-202401132034-128-5e-3/tmp-checkpoint-200/special_tokens_map.json
/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 67%|██████▋   | 201/300 [1:16:53<37:55, 22.98s/it]                                                   {'loss': 0.5281, 'learning_rate': 0.0016500000000000002, 'epoch': 21.02}
 67%|██████▋   | 201/300 [1:16:53<37:55, 22.98s/it] 67%|██████▋   | 202/300 [1:17:16<37:33, 22.99s/it]                                                   {'loss': 0.5155, 'learning_rate': 0.0016333333333333334, 'epoch': 21.12}
 67%|██████▋   | 202/300 [1:17:16<37:33, 22.99s/it] 68%|██████▊   | 203/300 [1:17:38<37:09, 22.99s/it]                                                   {'loss': 0.5397, 'learning_rate': 0.0016166666666666666, 'epoch': 21.23}
 68%|██████▊   | 203/300 [1:17:38<37:09, 22.99s/it] 68%|██████▊   | 204/300 [1:18:01<36:46, 22.99s/it]                                                   {'loss': 0.4916, 'learning_rate': 0.0016, 'epoch': 21.33}
 68%|██████▊   | 204/300 [1:18:01<36:46, 22.99s/it] 68%|██████▊   | 205/300 [1:18:24<36:23, 22.99s/it]                                                   {'loss': 0.483, 'learning_rate': 0.0015833333333333333, 'epoch': 21.44}
 68%|██████▊   | 205/300 [1:18:24<36:23, 22.99s/it] 69%|██████▊   | 206/300 [1:18:47<35:59, 22.98s/it]                                                   {'loss': 0.486, 'learning_rate': 0.0015666666666666667, 'epoch': 21.54}
 69%|██████▊   | 206/300 [1:18:47<35:59, 22.98s/it] 69%|██████▉   | 207/300 [1:19:10<35:35, 22.97s/it]                                                   {'loss': 0.4968, 'learning_rate': 0.00155, 'epoch': 21.65}
 69%|██████▉   | 207/300 [1:19:10<35:35, 22.97s/it] 69%|██████▉   | 208/300 [1:19:33<35:13, 22.97s/it]                                                   {'loss': 0.4343, 'learning_rate': 0.0015333333333333332, 'epoch': 21.75}
 69%|██████▉   | 208/300 [1:19:33<35:13, 22.97s/it] 70%|██████▉   | 209/300 [1:19:56<34:49, 22.97s/it]                                                   {'loss': 0.5526, 'learning_rate': 0.0015166666666666668, 'epoch': 21.86}
 70%|██████▉   | 209/300 [1:19:56<34:49, 22.97s/it] 70%|███████   | 210/300 [1:20:19<34:26, 22.96s/it]                                                   {'loss': 0.5304, 'learning_rate': 0.0015, 'epoch': 21.96}
 70%|███████   | 210/300 [1:20:19<34:26, 22.96s/it] 70%|███████   | 211/300 [1:20:42<34:03, 22.97s/it]                                                   {'loss': 0.5663, 'learning_rate': 0.0014833333333333335, 'epoch': 22.07}
 70%|███████   | 211/300 [1:20:42<34:03, 22.97s/it] 71%|███████   | 212/300 [1:21:05<33:41, 22.97s/it]                                                   {'loss': 0.4891, 'learning_rate': 0.0014666666666666667, 'epoch': 22.17}
 71%|███████   | 212/300 [1:21:05<33:41, 22.97s/it] 71%|███████   | 213/300 [1:21:28<33:17, 22.96s/it]                                                   {'loss': 0.5312, 'learning_rate': 0.00145, 'epoch': 22.27}
 71%|███████   | 213/300 [1:21:28<33:17, 22.96s/it] 71%|███████▏  | 214/300 [1:21:51<32:55, 22.97s/it]                                                   {'loss': 0.497, 'learning_rate': 0.0014333333333333333, 'epoch': 22.38}
 71%|███████▏  | 214/300 [1:21:51<32:55, 22.97s/it] 72%|███████▏  | 215/300 [1:22:14<32:31, 22.96s/it]                                                   {'loss': 0.5009, 'learning_rate': 0.0014166666666666666, 'epoch': 22.48}
 72%|███████▏  | 215/300 [1:22:14<32:31, 22.96s/it] 72%|███████▏  | 216/300 [1:22:37<32:09, 22.97s/it]                                                   {'loss': 0.4887, 'learning_rate': 0.0014000000000000002, 'epoch': 22.59}
 72%|███████▏  | 216/300 [1:22:37<32:09, 22.97s/it] 72%|███████▏  | 217/300 [1:23:00<31:46, 22.97s/it]                                                   {'loss': 0.5661, 'learning_rate': 0.0013833333333333334, 'epoch': 22.69}
 72%|███████▏  | 217/300 [1:23:00<31:46, 22.97s/it] 73%|███████▎  | 218/300 [1:23:23<31:22, 22.96s/it]                                                   {'loss': 0.4294, 'learning_rate': 0.0013666666666666666, 'epoch': 22.8}
 73%|███████▎  | 218/300 [1:23:23<31:22, 22.96s/it] 73%|███████▎  | 219/300 [1:23:46<30:59, 22.96s/it]                                                   {'loss': 0.4282, 'learning_rate': 0.00135, 'epoch': 22.9}
 73%|███████▎  | 219/300 [1:23:46<30:59, 22.96s/it] 73%|███████▎  | 220/300 [1:24:09<30:37, 22.97s/it]                                                   {'loss': 0.4921, 'learning_rate': 0.0013333333333333333, 'epoch': 23.01}
 73%|███████▎  | 220/300 [1:24:09<30:37, 22.97s/it] 74%|███████▎  | 221/300 [1:24:32<30:14, 22.96s/it]                                                   {'loss': 0.4576, 'learning_rate': 0.0013166666666666665, 'epoch': 23.11}
 74%|███████▎  | 221/300 [1:24:32<30:14, 22.96s/it] 74%|███████▍  | 222/300 [1:24:55<29:51, 22.96s/it]                                                   {'loss': 0.4504, 'learning_rate': 0.0013000000000000002, 'epoch': 23.22}
 74%|███████▍  | 222/300 [1:24:55<29:51, 22.96s/it] 74%|███████▍  | 223/300 [1:25:18<29:28, 22.97s/it]                                                   {'loss': 0.4593, 'learning_rate': 0.0012833333333333334, 'epoch': 23.32}
 74%|███████▍  | 223/300 [1:25:18<29:28, 22.97s/it] 75%|███████▍  | 224/300 [1:25:41<29:05, 22.96s/it]                                                   {'loss': 0.5027, 'learning_rate': 0.0012666666666666668, 'epoch': 23.42}
 75%|███████▍  | 224/300 [1:25:41<29:05, 22.96s/it] 75%|███████▌  | 225/300 [1:26:04<28:42, 22.97s/it]                                                   {'loss': 0.4825, 'learning_rate': 0.00125, 'epoch': 23.53}
 75%|███████▌  | 225/300 [1:26:04<28:42, 22.97s/it] 75%|███████▌  | 226/300 [1:26:27<28:20, 22.97s/it]                                                   {'loss': 0.5421, 'learning_rate': 0.0012333333333333335, 'epoch': 23.63}
 75%|███████▌  | 226/300 [1:26:27<28:20, 22.97s/it] 76%|███████▌  | 227/300 [1:26:50<27:56, 22.97s/it]                                                   {'loss': 0.5588, 'learning_rate': 0.0012166666666666667, 'epoch': 23.74}
 76%|███████▌  | 227/300 [1:26:50<27:56, 22.97s/it] 76%|███████▌  | 228/300 [1:27:13<27:33, 22.97s/it]                                                   {'loss': 0.5248, 'learning_rate': 0.0012, 'epoch': 23.84}
 76%|███████▌  | 228/300 [1:27:13<27:33, 22.97s/it] 76%|███████▋  | 229/300 [1:27:36<27:10, 22.97s/it]                                                   {'loss': 0.4736, 'learning_rate': 0.0011833333333333333, 'epoch': 23.95}
 76%|███████▋  | 229/300 [1:27:36<27:10, 22.97s/it] 77%|███████▋  | 230/300 [1:27:59<26:48, 22.97s/it]                                                   {'loss': 0.5111, 'learning_rate': 0.0011666666666666668, 'epoch': 24.05}
 77%|███████▋  | 230/300 [1:27:59<26:48, 22.97s/it] 77%|███████▋  | 231/300 [1:28:22<26:25, 22.97s/it]                                                   {'loss': 0.5083, 'learning_rate': 0.00115, 'epoch': 24.16}
 77%|███████▋  | 231/300 [1:28:22<26:25, 22.97s/it] 77%|███████▋  | 232/300 [1:28:45<26:02, 22.98s/it]                                                   {'loss': 0.5972, 'learning_rate': 0.0011333333333333332, 'epoch': 24.26}
 77%|███████▋  | 232/300 [1:28:45<26:02, 22.98s/it] 78%|███████▊  | 233/300 [1:29:08<25:39, 22.98s/it]                                                   {'loss': 0.4734, 'learning_rate': 0.0011166666666666666, 'epoch': 24.37}
 78%|███████▊  | 233/300 [1:29:08<25:39, 22.98s/it] 78%|███████▊  | 234/300 [1:29:31<25:17, 22.99s/it]                                                   {'loss': 0.5402, 'learning_rate': 0.0011, 'epoch': 24.47}
 78%|███████▊  | 234/300 [1:29:31<25:17, 22.99s/it] 78%|███████▊  | 235/300 [1:29:54<24:54, 22.99s/it]                                                   {'loss': 0.4365, 'learning_rate': 0.0010833333333333335, 'epoch': 24.58}
 78%|███████▊  | 235/300 [1:29:54<24:54, 22.99s/it] 79%|███████▊  | 236/300 [1:30:17<24:30, 22.98s/it]                                                   {'loss': 0.4338, 'learning_rate': 0.0010666666666666667, 'epoch': 24.68}
 79%|███████▊  | 236/300 [1:30:17<24:30, 22.98s/it] 79%|███████▉  | 237/300 [1:30:39<24:07, 22.97s/it]                                                   {'loss': 0.5372, 'learning_rate': 0.00105, 'epoch': 24.78}
 79%|███████▉  | 237/300 [1:30:39<24:07, 22.97s/it] 79%|███████▉  | 238/300 [1:31:02<23:43, 22.97s/it]                                                   {'loss': 0.4547, 'learning_rate': 0.0010333333333333334, 'epoch': 24.89}
 79%|███████▉  | 238/300 [1:31:02<23:43, 22.97s/it] 80%|███████▉  | 239/300 [1:31:25<23:20, 22.96s/it]                                                   {'loss': 0.4826, 'learning_rate': 0.0010166666666666668, 'epoch': 24.99}
 80%|███████▉  | 239/300 [1:31:25<23:20, 22.96s/it] 80%|████████  | 240/300 [1:31:48<22:57, 22.96s/it]                                                   {'loss': 0.474, 'learning_rate': 0.001, 'epoch': 25.1}
 80%|████████  | 240/300 [1:31:48<22:57, 22.96s/it] 80%|████████  | 241/300 [1:32:11<22:34, 22.96s/it]                                                   {'loss': 0.4955, 'learning_rate': 0.0009833333333333332, 'epoch': 25.2}
 80%|████████  | 241/300 [1:32:11<22:34, 22.96s/it] 81%|████████  | 242/300 [1:32:34<22:11, 22.96s/it]                                                   {'loss': 0.4348, 'learning_rate': 0.0009666666666666667, 'epoch': 25.31}
 81%|████████  | 242/300 [1:32:34<22:11, 22.96s/it] 81%|████████  | 243/300 [1:32:57<21:48, 22.96s/it]                                                   {'loss': 0.4223, 'learning_rate': 0.00095, 'epoch': 25.41}
 81%|████████  | 243/300 [1:32:57<21:48, 22.96s/it] 81%|████████▏ | 244/300 [1:33:20<21:26, 22.97s/it]                                                   {'loss': 0.4447, 'learning_rate': 0.0009333333333333334, 'epoch': 25.52}
 81%|████████▏ | 244/300 [1:33:20<21:26, 22.97s/it] 82%|████████▏ | 245/300 [1:33:43<21:02, 22.96s/it]                                                   {'loss': 0.4948, 'learning_rate': 0.0009166666666666666, 'epoch': 25.62}
 82%|████████▏ | 245/300 [1:33:43<21:02, 22.96s/it] 82%|████████▏ | 246/300 [1:34:06<20:40, 22.97s/it]                                                   {'loss': 0.5695, 'learning_rate': 0.0009, 'epoch': 25.73}
 82%|████████▏ | 246/300 [1:34:06<20:40, 22.97s/it] 82%|████████▏ | 247/300 [1:34:29<20:17, 22.97s/it]                                                   {'loss': 0.5121, 'learning_rate': 0.0008833333333333333, 'epoch': 25.83}
 82%|████████▏ | 247/300 [1:34:29<20:17, 22.97s/it] 83%|████████▎ | 248/300 [1:34:52<19:54, 22.97s/it]                                                   {'loss': 0.6011, 'learning_rate': 0.0008666666666666667, 'epoch': 25.93}
 83%|████████▎ | 248/300 [1:34:52<19:54, 22.97s/it] 83%|████████▎ | 249/300 [1:35:15<19:31, 22.98s/it]                                                   {'loss': 0.4629, 'learning_rate': 0.0008500000000000001, 'epoch': 26.04}
 83%|████████▎ | 249/300 [1:35:15<19:31, 22.98s/it] 83%|████████▎ | 250/300 [1:35:38<19:08, 22.97s/it]                                                   {'loss': 0.4542, 'learning_rate': 0.0008333333333333333, 'epoch': 26.14}
 83%|████████▎ | 250/300 [1:35:38<19:08, 22.97s/it] 84%|████████▎ | 251/300 [1:36:01<18:44, 22.96s/it]                                                   {'loss': 0.4952, 'learning_rate': 0.0008166666666666667, 'epoch': 26.25}
 84%|████████▎ | 251/300 [1:36:01<18:44, 22.96s/it] 84%|████████▍ | 252/300 [1:36:24<18:22, 22.96s/it]                                                   {'loss': 0.4654, 'learning_rate': 0.0008, 'epoch': 26.35}
 84%|████████▍ | 252/300 [1:36:24<18:22, 22.96s/it] 84%|████████▍ | 253/300 [1:36:47<17:59, 22.97s/it]                                                   {'loss': 0.5277, 'learning_rate': 0.0007833333333333334, 'epoch': 26.46}
 84%|████████▍ | 253/300 [1:36:47<17:59, 22.97s/it] 85%|████████▍ | 254/300 [1:37:10<17:36, 22.96s/it]                                                   {'loss': 0.4766, 'learning_rate': 0.0007666666666666666, 'epoch': 26.56}
 85%|████████▍ | 254/300 [1:37:10<17:36, 22.96s/it] 85%|████████▌ | 255/300 [1:37:33<17:13, 22.96s/it]                                                   {'loss': 0.45, 'learning_rate': 0.00075, 'epoch': 26.67}
 85%|████████▌ | 255/300 [1:37:33<17:13, 22.96s/it] 85%|████████▌ | 256/300 [1:37:56<16:50, 22.96s/it]                                                   {'loss': 0.479, 'learning_rate': 0.0007333333333333333, 'epoch': 26.77}
 85%|████████▌ | 256/300 [1:37:56<16:50, 22.96s/it] 86%|████████▌ | 257/300 [1:38:19<16:27, 22.96s/it]                                                   {'loss': 0.5554, 'learning_rate': 0.0007166666666666667, 'epoch': 26.88}
 86%|████████▌ | 257/300 [1:38:19<16:27, 22.96s/it] 86%|████████▌ | 258/300 [1:38:42<16:04, 22.97s/it]                                                   {'loss': 0.4729, 'learning_rate': 0.0007000000000000001, 'epoch': 26.98}
 86%|████████▌ | 258/300 [1:38:42<16:04, 22.97s/it] 86%|████████▋ | 259/300 [1:39:05<15:42, 22.98s/it]                                                   {'loss': 0.4953, 'learning_rate': 0.0006833333333333333, 'epoch': 27.08}
 86%|████████▋ | 259/300 [1:39:05<15:42, 22.98s/it] 87%|████████▋ | 260/300 [1:39:28<15:18, 22.97s/it]                                                   {'loss': 0.4833, 'learning_rate': 0.0006666666666666666, 'epoch': 27.19}
 87%|████████▋ | 260/300 [1:39:28<15:18, 22.97s/it] 87%|████████▋ | 261/300 [1:39:51<14:55, 22.97s/it]                                                   {'loss': 0.4734, 'learning_rate': 0.0006500000000000001, 'epoch': 27.29}
 87%|████████▋ | 261/300 [1:39:51<14:55, 22.97s/it] 87%|████████▋ | 262/300 [1:40:14<14:32, 22.96s/it]                                                   {'loss': 0.4907, 'learning_rate': 0.0006333333333333334, 'epoch': 27.4}
 87%|████████▋ | 262/300 [1:40:14<14:32, 22.96s/it] 88%|████████▊ | 263/300 [1:40:37<14:09, 22.97s/it]                                                   {'loss': 0.4948, 'learning_rate': 0.0006166666666666667, 'epoch': 27.5}
 88%|████████▊ | 263/300 [1:40:37<14:09, 22.97s/it] 88%|████████▊ | 264/300 [1:41:00<13:46, 22.96s/it]                                                   {'loss': 0.5135, 'learning_rate': 0.0006, 'epoch': 27.61}
 88%|████████▊ | 264/300 [1:41:00<13:46, 22.96s/it] 88%|████████▊ | 265/300 [1:41:22<13:23, 22.96s/it]                                                   {'loss': 0.5501, 'learning_rate': 0.0005833333333333334, 'epoch': 27.71}
 88%|████████▊ | 265/300 [1:41:22<13:23, 22.96s/it] 89%|████████▊ | 266/300 [1:41:45<13:00, 22.97s/it]                                                   {'loss': 0.4785, 'learning_rate': 0.0005666666666666666, 'epoch': 27.82}
 89%|████████▊ | 266/300 [1:41:45<13:00, 22.97s/it] 89%|████████▉ | 267/300 [1:42:08<12:37, 22.97s/it]                                                   {'loss': 0.4716, 'learning_rate': 0.00055, 'epoch': 27.92}
 89%|████████▉ | 267/300 [1:42:08<12:37, 22.97s/it] 89%|████████▉ | 268/300 [1:42:31<12:14, 22.96s/it]                                                   {'loss': 0.4562, 'learning_rate': 0.0005333333333333334, 'epoch': 28.03}
 89%|████████▉ | 268/300 [1:42:31<12:14, 22.96s/it] 90%|████████▉ | 269/300 [1:42:54<11:52, 22.97s/it]                                                   {'loss': 0.494, 'learning_rate': 0.0005166666666666667, 'epoch': 28.13}
 90%|████████▉ | 269/300 [1:42:54<11:52, 22.97s/it] 90%|█████████ | 270/300 [1:43:17<11:28, 22.96s/it]                                                   {'loss': 0.4631, 'learning_rate': 0.0005, 'epoch': 28.24}
 90%|█████████ | 270/300 [1:43:17<11:28, 22.96s/it] 90%|█████████ | 271/300 [1:43:40<11:06, 22.97s/it]                                                   {'loss': 0.4915, 'learning_rate': 0.00048333333333333334, 'epoch': 28.34}
 90%|█████████ | 271/300 [1:43:40<11:06, 22.97s/it] 91%|█████████ | 272/300 [1:44:03<10:43, 22.98s/it]                                                   {'loss': 0.4478, 'learning_rate': 0.0004666666666666667, 'epoch': 28.44}
 91%|█████████ | 272/300 [1:44:03<10:43, 22.98s/it] 91%|█████████ | 273/300 [1:44:26<10:20, 22.98s/it]                                                   {'loss': 0.4931, 'learning_rate': 0.00045, 'epoch': 28.55}
 91%|█████████ | 273/300 [1:44:26<10:20, 22.98s/it] 91%|█████████▏| 274/300 [1:44:49<09:57, 22.97s/it]                                                   {'loss': 0.5325, 'learning_rate': 0.00043333333333333337, 'epoch': 28.65}
 91%|█████████▏| 274/300 [1:44:49<09:57, 22.97s/it] 92%|█████████▏| 275/300 [1:45:12<09:34, 22.99s/it]                                                   {'loss': 0.4828, 'learning_rate': 0.00041666666666666664, 'epoch': 28.76}
 92%|█████████▏| 275/300 [1:45:12<09:34, 22.99s/it] 92%|█████████▏| 276/300 [1:45:35<09:11, 22.98s/it]                                                   {'loss': 0.4734, 'learning_rate': 0.0004, 'epoch': 28.86}
 92%|█████████▏| 276/300 [1:45:35<09:11, 22.98s/it] 92%|█████████▏| 277/300 [1:45:58<08:48, 22.97s/it]                                                   {'loss': 0.4817, 'learning_rate': 0.0003833333333333333, 'epoch': 28.97}
 92%|█████████▏| 277/300 [1:45:58<08:48, 22.97s/it] 93%|█████████▎| 278/300 [1:46:21<08:25, 22.98s/it]                                                   {'loss': 0.5336, 'learning_rate': 0.00036666666666666667, 'epoch': 29.07}
 93%|█████████▎| 278/300 [1:46:21<08:25, 22.98s/it] 93%|█████████▎| 279/300 [1:46:44<08:02, 22.97s/it]                                                   {'loss': 0.4956, 'learning_rate': 0.00035000000000000005, 'epoch': 29.18}
 93%|█████████▎| 279/300 [1:46:44<08:02, 22.97s/it] 93%|█████████▎| 280/300 [1:47:07<07:39, 22.97s/it]                                                   {'loss': 0.4317, 'learning_rate': 0.0003333333333333333, 'epoch': 29.28}
 93%|█████████▎| 280/300 [1:47:07<07:39, 22.97s/it] 94%|█████████▎| 281/300 [1:47:30<07:16, 22.97s/it]                                                   {'loss': 0.505, 'learning_rate': 0.0003166666666666667, 'epoch': 29.39}
 94%|█████████▎| 281/300 [1:47:30<07:16, 22.97s/it] 94%|█████████▍| 282/300 [1:47:53<06:53, 22.97s/it]                                                   {'loss': 0.4928, 'learning_rate': 0.0003, 'epoch': 29.49}
 94%|█████████▍| 282/300 [1:47:53<06:53, 22.97s/it] 94%|█████████▍| 283/300 [1:48:16<06:30, 22.97s/it]                                                   {'loss': 0.5488, 'learning_rate': 0.0002833333333333333, 'epoch': 29.59}
 94%|█████████▍| 283/300 [1:48:16<06:30, 22.97s/it] 95%|█████████▍| 284/300 [1:48:39<06:07, 22.98s/it]                                                   {'loss': 0.4599, 'learning_rate': 0.0002666666666666667, 'epoch': 29.7}
 95%|█████████▍| 284/300 [1:48:39<06:07, 22.98s/it] 95%|█████████▌| 285/300 [1:49:02<05:44, 22.97s/it]                                                   {'loss': 0.4347, 'learning_rate': 0.00025, 'epoch': 29.8}
 95%|█████████▌| 285/300 [1:49:02<05:44, 22.97s/it] 95%|█████████▌| 286/300 [1:49:25<05:21, 22.97s/it]                                                   {'loss': 0.4953, 'learning_rate': 0.00023333333333333336, 'epoch': 29.91}
 95%|█████████▌| 286/300 [1:49:25<05:21, 22.97s/it] 96%|█████████▌| 287/300 [1:49:48<04:58, 22.96s/it]                                                   {'loss': 0.4526, 'learning_rate': 0.00021666666666666668, 'epoch': 30.01}
 96%|█████████▌| 287/300 [1:49:48<04:58, 22.96s/it] 96%|█████████▌| 288/300 [1:50:11<04:35, 22.96s/it]                                                   {'loss': 0.4794, 'learning_rate': 0.0002, 'epoch': 30.12}
 96%|█████████▌| 288/300 [1:50:11<04:35, 22.96s/it] 96%|█████████▋| 289/300 [1:50:34<04:12, 22.96s/it]                                                   {'loss': 0.4398, 'learning_rate': 0.00018333333333333334, 'epoch': 30.22}
 96%|█████████▋| 289/300 [1:50:34<04:12, 22.96s/it] 97%|█████████▋| 290/300 [1:50:57<03:49, 22.97s/it]                                                   {'loss': 0.4838, 'learning_rate': 0.00016666666666666666, 'epoch': 30.33}
 97%|█████████▋| 290/300 [1:50:57<03:49, 22.97s/it] 97%|█████████▋| 291/300 [1:51:20<03:26, 22.96s/it]                                                   {'loss': 0.4937, 'learning_rate': 0.00015, 'epoch': 30.43}
 97%|█████████▋| 291/300 [1:51:20<03:26, 22.96s/it] 97%|█████████▋| 292/300 [1:51:43<03:03, 22.96s/it]                                                   {'loss': 0.488, 'learning_rate': 0.00013333333333333334, 'epoch': 30.54}
 97%|█████████▋| 292/300 [1:51:43<03:03, 22.96s/it] 98%|█████████▊| 293/300 [1:52:06<02:40, 22.96s/it]                                                   {'loss': 0.4807, 'learning_rate': 0.00011666666666666668, 'epoch': 30.64}
 98%|█████████▊| 293/300 [1:52:06<02:40, 22.96s/it] 98%|█████████▊| 294/300 [1:52:29<02:17, 22.96s/it]                                                   {'loss': 0.5034, 'learning_rate': 0.0001, 'epoch': 30.75}
 98%|█████████▊| 294/300 [1:52:29<02:17, 22.96s/it] 98%|█████████▊| 295/300 [1:52:52<01:54, 22.96s/it]                                                   {'loss': 0.495, 'learning_rate': 8.333333333333333e-05, 'epoch': 30.85}
 98%|█████████▊| 295/300 [1:52:52<01:54, 22.96s/it] 99%|█████████▊| 296/300 [1:53:15<01:31, 22.97s/it]                                                   {'loss': 0.5048, 'learning_rate': 6.666666666666667e-05, 'epoch': 30.95}
 99%|█████████▊| 296/300 [1:53:15<01:31, 22.97s/it] 99%|█████████▉| 297/300 [1:53:37<01:08, 22.96s/it]                                                   {'loss': 0.4982, 'learning_rate': 5e-05, 'epoch': 31.06}
 99%|█████████▉| 297/300 [1:53:37<01:08, 22.96s/it] 99%|█████████▉| 298/300 [1:54:00<00:45, 22.96s/it]                                                   {'loss': 0.4469, 'learning_rate': 3.3333333333333335e-05, 'epoch': 31.16}
 99%|█████████▉| 298/300 [1:54:00<00:45, 22.96s/it]100%|█████████▉| 299/300 [1:54:23<00:22, 22.97s/it]                                                   {'loss': 0.5316, 'learning_rate': 1.6666666666666667e-05, 'epoch': 31.27}
100%|█████████▉| 299/300 [1:54:23<00:22, 22.97s/it]100%|██████████| 300/300 [1:54:46<00:00, 22.96s/it]                                                   {'loss': 0.498, 'learning_rate': 0.0, 'epoch': 31.37}
100%|██████████| 300/300 [1:54:46<00:00, 22.96s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:483] 2024-01-13 22:37:21,303 >> Configuration saved in output/test-202401132034-128-5e-3/tmp-checkpoint-300/config.json
[INFO|configuration_utils.py:594] 2024-01-13 22:37:21,303 >> Configuration saved in output/test-202401132034-128-5e-3/tmp-checkpoint-300/generation_config.json
[INFO|modeling_utils.py:2382] 2024-01-13 22:37:21,313 >> Model weights saved in output/test-202401132034-128-5e-3/tmp-checkpoint-300/pytorch_model.bin
[INFO|tokenization_utils_base.py:2432] 2024-01-13 22:37:21,314 >> tokenizer config file saved in output/test-202401132034-128-5e-3/tmp-checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-01-13 22:37:21,314 >> Special tokens file saved in output/test-202401132034-128-5e-3/tmp-checkpoint-300/special_tokens_map.json
[INFO|trainer.py:1947] 2024-01-13 22:37:21,343 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   {'train_runtime': 6886.9352, 'train_samples_per_second': 0.697, 'train_steps_per_second': 0.044, 'train_loss': 0.6289674123128255, 'epoch': 31.37}
100%|██████████| 300/300 [1:54:46<00:00, 22.96s/it]100%|██████████| 300/300 [1:54:46<00:00, 22.96s/it]
Saving PrefixEncoder
[INFO|configuration_utils.py:483] 2024-01-13 22:37:21,351 >> Configuration saved in output/test-202401132034-128-5e-3/config.json
[INFO|configuration_utils.py:594] 2024-01-13 22:37:21,352 >> Configuration saved in output/test-202401132034-128-5e-3/generation_config.json
[INFO|modeling_utils.py:2382] 2024-01-13 22:37:21,361 >> Model weights saved in output/test-202401132034-128-5e-3/pytorch_model.bin
[INFO|tokenization_utils_base.py:2432] 2024-01-13 22:37:21,362 >> tokenizer config file saved in output/test-202401132034-128-5e-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-01-13 22:37:21,362 >> Special tokens file saved in output/test-202401132034-128-5e-3/special_tokens_map.json
